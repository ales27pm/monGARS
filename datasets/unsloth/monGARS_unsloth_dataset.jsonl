{
  "prompt": "Map the communication contract for /api/v1/conversation/chat when a signed-in user submits a message. Detail every module hand-off and expected logging touchpoint.",
  "completion": "[[monGARS-protocol]]\nsystem_guidance:\n  - Enforce rate limiting before any cognition work.\n  - Sanitize payloads with security validators to strip HTML/script tags.\nmodule_chain:\n  - module: monGARS.api.web_api.chat\n    responsibilities:\n      - Invoke InMemoryRateLimiter.ensure_permitted for current_user.sub.\n      - Normalize and validate the request through validate_user_input.\n    io:\n      receives: FastAPI ChatRequest, bearer-authenticated user.\n      emits: Cleaned query string for cognition stack.\n  - module: monGARS.core.conversation.ConversationalModule.generate_response\n    responsibilities:\n      - Build enriched prompt, enforce PromptTooLargeError safeguards, call LLMIntegration.generate_response.\n      - Persist interaction telemetry and speech payloads.\n    io:\n      receives: sanitized query, session_id.\n      emits: text response, confidence, processing_time, speech_turn payload.\n  - module: monGARS.core.ui_events.event_bus\n    responsibilities:\n      - Publish chat.message event with timestamped payload.\n    io:\n      receives: response envelope {query,response,timestamp,speech_turn}.\n      emits: async broadcast to subscribers; tolerate BackendUnavailable.\nuser_interface:\n  entrypoint: HTTPS POST /api/v1/conversation/chat\n  message_contract:\n    - user_id derived from JWT subject; not user-supplied.\n    - message text validated + escaped via bleach.clean.\nobservability:\n  logs:\n    - web_api.chat_invalid_input (warning) on validation failures.\n    - web_api.chat_inference_failed (exception) when downstream raises.\n  metrics:\n    - HTTP_REQUEST_LATENCY_SECONDS histogram annotated with route template.\nerror_paths:\n  - PromptTooLargeError -> HTTP 413 with trimmed token metadata.\n  - asyncio.TimeoutError -> HTTP 504 with chat_inference_timeout log."}
{
  "prompt": "Describe the internal exchange when ConversationalModule handles a user prompt with an attached screenshot. Capture each adaptive enrichment stage in order.",
  "completion": "[[monGARS-protocol]]\nmodule_chain:\n  - stage: memory_bootstrap\n    module: monGARS.core.hippocampus.Hippocampus.history\n    detail: fetch up to 5 recent MemoryItem entries, pruning expired TTLs.\n  - stage: multimodal_caption\n    module: monGARS.core.conversation.ConversationalModule._handle_image\n    detail: call ImageCaptioning.generate_caption and append settings.caption_prefix output to the query.\n  - stage: curiosity_gap\n    module: monGARS.core.conversation.ConversationalModule._augment_with_curiosity\n    detail: CuriosityEngine.detect_gaps merges additional_context when status==insufficient_knowledge.\n  - stage: neuro_symbolic_refine\n    module: monGARS.core.conversation.ConversationalModule._refine_query\n    detail: AdvancedReasoner.reason returns metadata; invalid mappings trigger conversation.reasoner.invalid_result warning.\n  - stage: semantic_recall\n    module: monGARS.core.conversation.ConversationalModule._semantic_context_matches\n    detail: PersistenceRepository.vector_search_history gated by llm2vec_context_limit/max_distance settings.\n  - stage: prompt_composition\n    module: monGARS.core.conversation.ConversationalModule._compose_prompt\n    detail: build_converged_chat_prompt with trimmed history/semantic context respecting prompt_token_budget.\nuser_interface:\n  response_style:\n    - Preserve user intent; annotate added caption/curiosity context in operator logs only.\n    - Communicate reasoning augmentation transparently when result metadata contributes.\nobservability:\n  - Log conversation.prompt.trimmed with token math whenever history/semantic entries are dropped.\n  - Raise PromptTooLargeError when prompt_tokens exceed remaining budget after trimming.\nfallbacks:\n  - On caption failure, skip augmentation but continue pipeline.\n  - On curiosity timeout, proceed with base query without aborting."}
{
  "prompt": "Outline how ConversationalModule adapts the raw LLM response before returning data to the API. Highlight mimicry, speech synthesis, and persistence touchpoints.",
  "completion": "[[monGARS-protocol]]\npost_llm_chain:\n  - component: AdaptiveResponseGenerator\n    actions:\n      - get_personality_traits(user_id, recent_interactions) with TTL-cached traits.\n      - generate_adaptive_response(text, personality, user_id=user_id).\n  - component: MimicryModule\n    actions:\n      - update_profile(user_id, {message: original_query, response: text}).\n      - adapt_response_style(adaptive_text, user_id) to finalize voice.\n  - component: SpeakerService\n    actions:\n      - speak(styled_text, session_id=session_id or user_id) returning speech_turn payload.\n  - component: PersistenceRepository.save_interaction\n    actions:\n      - Persist Interaction with input_data (queries, reasoning metadata, semantic context) and output_data (raw_llm, adapted_text, speech_turn).\n      - Include llm prompt/generation token bookkeeping for analytics.\n  - component: Hippocampus.store\n    actions:\n      - Append MemoryItem, compute TTL, persist via AsyncSession when configured.\n  - component: EvolutionEngine.record_memory_sample\n    actions:\n      - Capture curated sample if MemoryItem exists, skipping expired entries.\nuser_handback:\n  payload_keys:\n    - text: speech_turn.text\n    - confidence: llm_out.confidence defaulting to 0.0\n    - processing_time: UTC delta from start\n    - speech_turn: serialised audio metadata\noperator_notes:\n  - All mimicry failures surface as ValueError or logged exceptions without blocking persistence.\n  - Speech synthesis errors propagate via speaker exceptions; guard with retries upstream if needed."}
{
  "prompt": "Define the dialogue template the LLM should follow when the orchestrator pipeline, not the ConversationalModule, handles a lightweight query. Emphasize fallbacks and tone control.",
  "completion": "[[monGARS-protocol]]\nsystem_guidance:\n  - Respect system prompts injected by operators via settings.caption_prefix and downstream tone controls.\n  - Keep acknowledgements concise when modules fall back to defaults.\nmodule_chain:\n  - CuriosityEngine.detect_gaps -> append additional_context when status==insufficient_knowledge.\n  - AdvancedReasoner.reason -> include reasoning result inline if present.\n  - LLMIntegration.generate_response -> provide base_response and confidence, tolerate CircuitBreaker errors.\n  - AdaptiveResponseGenerator.generate_adaptive_response -> apply personality traits (or {} on failure).\n  - MimicryModule.update_profile/adapt_response_style -> ensure stylistic continuity; log mimicry failures without aborting.\nuser_voice_template:\n  - Opening: Empathetic acknowledgement referencing refined_query.\n  - Body: Structured bullet or numbered guidance rooted in reasoning metadata.\n  - Closing: Offer follow-up invite with mimicry-preserved tone.\nfallbacks:\n  - Caption failures -> proceed with original query, note omission in logs only.\n  - Reasoner exceptions -> use unrefined query but keep friendly closing.\nobservability:\n  - log_interaction truncates query/response to 100 chars with DEBUG severity for traceability."}
{
  "prompt": "Document the standard operating procedure for storing memory after a chat response so that downstream self-improvement pipelines stay consistent.",
  "completion": "[[monGARS-protocol]]\nworkflow:\n  1. PersistenceRepository.save_interaction persists Interaction with message, response, personality, history, semantic context, and token budgets.\n  2. Hippocampus.store:\n     - Acquire per-user asyncio.Lock.\n     - Append MemoryItem(user_id, query, response, timestamp, expires_at).\n     - Persist to SQL via session_scope if async_session_factory available; warn on failure.\n  3. EvolutionEngine.record_memory_sample:\n     - Skip expired samples.\n     - Append curated payload under asyncio.Lock for later curation.\n  4. EvolutionEngine orchestrator ingests curated samples when ReinforcementLoop triggers new adapter training.\ncommunication_contract:\n  - No user-visible message; operate silently post-response.\n  - Operators can audit via evolution.memory.record DEBUG logs.\nresilience:\n  - Persistence errors logged via hippocampus.persistence_failed without breaking flow.\n  - Evolution engine gracefully skips scheduling when expires_at <= now."}
{
  "prompt": "Explain how user input is sanitized and validated prior to invoking cognition stacks. Provide the format the model should echo when confirming sanitized fields back to the user.",
  "completion": "[[monGARS-protocol]]\nsanitization_pipeline:\n  - monGARS.api.web_api.chat calls validate_user_input with {user_id, query}.\n  - monGARS.core.security.validate_user_input iterates values, applying bleach.clean(strip=True).\n  - Missing user_id or query raises ValueError -> surfaced as HTTP 422.\nconfirmation_template:\n  user_input_ack:\n    - user_id: derive from authenticated token; never echo raw token.\n    - query_preview: first 120 chars of sanitized query with HTML removed.\n    - safety_note: \"Content sanitized to remove HTML/JS before processing.\"\nmodule_guidance:\n  - Never reflect raw unsanitized message.\n  - Mention that further enrichment (curiosity, reasoning) occurs after validation.\nerror_messaging:\n  - On validation failure respond: \"I couldn’t process that request because required fields were missing after sanitization.\""}
{
  "prompt": "When semantic recall is enabled, how should the model describe the additional context it surfaced from memory to the user without leaking raw telemetry?",
  "completion": "[[monGARS-protocol]]\nsemantic_context_brief:\n  - Source: PersistenceRepository.vector_search_history results filtered by llm2vec_context_limit/max_distance.\n  - Sanitization: include only query/response summaries, omit internal IDs or similarity scores.\nuser-facing_format:\n  - Intro sentence: \"I’ve pulled a relevant memory from our previous chats to keep things consistent.\"\n  - Bullet per match: \"• Prior topic: <paraphrased_query> — I noted <paraphrased_response>.\"\n  - Close: \"Let me know if this context should be updated.\"\nmodule_notes:\n  - Internally preserve VectorMatch.distance + similarity for persistence analytics, but do not expose numeric values.\n  - If vector search fails, omit this section entirely."}
{
  "prompt": "Specify the expected conversational output when PromptTooLargeError is triggered after trimming history and semantic matches.",
  "completion": "[[monGARS-protocol]]\ntrigger_conditions:\n  - prompt_tokens remains > prompt_token_budget after iterative trimming of history and semantic context.\nuser_message:\n  - Opening apology acknowledging size limit.\n  - Provide actionable guidance: request summarization or splitting of request.\n  - Include token guidance from llm_out metadata if available.\noperator_logging:\n  - Ensure conversation.prompt.exceeds_limit warning captured with user hash, prompt_tokens, token_limit, reservation metadata.\nfollow_up_offer:\n  - Suggest continuing with abbreviated context; maintain friendly tone consistent with AdaptiveResponseGenerator traits."}
{
  "prompt": "Describe how peer telemetry and chat completions coexist so that LLM fine-tuning preserves admin messaging conventions.",
  "completion": "[[monGARS-protocol]]\nmodules_involved:\n  - monGARS.api.web_api.peer_message / peer_register / peer_load: manage encrypted peer payloads and registry.\n  - monGARS.core.peer.PeerCommunicator: decode payloads, track peer URLs, expose telemetry snapshots.\n  - monGARS.api.web_api.chat: continues to publish chat.message events even while peer telemetry endpoints operate.\nuser_vs_admin_channels:\n  - Standard users interact only with /conversation/chat responses.\n  - Admin operators receive peer telemetry feedback via secured endpoints; LLM acknowledgements must avoid referencing internal peer URLs.\ncommunication_style:\n  - When informing admins about peer broadcasts, summarize counts (\"3 peers registered\") rather than raw URLs.\n  - For regular users, never expose peer telemetry fields; keep focus on conversation content."}
{
  "prompt": "Provide the communication scaffold the model should use when evolution diagnostics detect resource pressure and a user asks about system health mid-conversation.",
  "completion": "[[monGARS-protocol]]\ninputs:\n  - EvolutionEngine.diagnose_performance returns PerformanceIssue entries (identifier, severity, details).\n  - SystemMonitor stats aggregate CPU, memory, GPU, disk usage.\nresponse_structure:\n  - Acknowledge user question.\n  - Summarize active issues grouped by severity (e.g., \"High CPU load\").\n  - Offer mitigation promise referencing evolution workflows (e.g., scaling via EvolutionOrchestrator).\n  - Reinforce continued availability and invite follow-up.\nmodule_cooperation:\n  - EvolutionEngine feeds curated samples + diagnostics to EvolutionOrchestrator, which may adjust Ray deployments and training slots.\n  - Keep user-facing text high level; raw wattage or replica counts remain internal."}
{
  "prompt": "Detail how the dataset should capture conversations where the user explicitly requests adapter updates, triggering self-training flows.",
  "completion": "[[monGARS-protocol]]\nscenario:\n  - User asks for improved behaviour or updated knowledge.\nmodule_chain:\n  - ConversationalModule persists interaction and stores memory sample.\n  - EvolutionEngine.curated_memory accumulates request and response with expiry semantics.\n  - EvolutionOrchestrator consumes curated data when schedule permits, using MNTPTrainer + ReinforcementLoop to produce adapters.\nuser_ack_template:\n  - Confirm request logged for self-improvement.\n  - Mention that curated data will inform future LoRA/adapter updates.\n  - Avoid promising immediate changes; reference operator approvals.\nobservability:\n  - Encourage logging evolution.memory.record with debug severity for traceability.\n  - Track training summaries via EvolutionOrchestrator energy + training_summary artifacts."}
{
  "prompt": "Summarize the proper messaging when event bus publication fails after generating a chat response so that retries or operator alerts can be initiated without alarming the user.",
  "completion": "[[monGARS-protocol]]\nfailure_context:\n  - Chat response already computed; make_event('chat.message', ...) raised BackendUnavailable or runtime error during event_bus().publish.\nuser-facing_guidance:\n  - Deliver chat response normally; do not mention telemetry hiccup.\noperator_playbook:\n  - Log web_api.chat_event_publish_failed with redacted user id.\n  - Emit WARN level alert to monitoring stack for manual retry.\n  - Optionally schedule background retry that replays response_payload to event bus when available.\nsafety_notes:\n  - Ensure response payload persists via PersistenceRepository before retry scheduling.\n  - Avoid duplicate user notifications; focus on backend observability."}

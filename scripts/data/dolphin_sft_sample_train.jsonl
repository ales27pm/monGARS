{"system": "You are Dolphin, an assistant that writes concise, technically accurate answers.", "prompt": "Explain why LoRA adapters are useful when fine-tuning large language models on consumer GPUs.", "response": "Low-Rank Adaptation (LoRA) freezes the base model weights and trains compact rank-update matrices. The smaller parameter set means updates fit into limited VRAM, converge quickly, and can be merged into different backbones without re-training the entire model."}
{"system": "You are Dolphin, an assistant that writes concise, technically accurate answers.", "prompt": "Provide two operational tips for preparing a Linux server for unattended fine-tuning runs.", "response": "Enable persistent logging so crashes are captured after reboots, and configure systemd targets or cron entries to relaunch the trainer automatically. Both steps reduce manual babysitting when the session drops."}

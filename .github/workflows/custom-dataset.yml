name: Build Custom Québec-French Dataset

on:
  workflow_dispatch:
    inputs:
      enable_external_fetch:
        description: "Fetch allowed external sources (Hansards / allow-listed crawl)"
        type: boolean
        default: false
      crawl_domains:
        description: "Newline-separated domains you OWN/have permission to crawl (e.g., https://example.qc.ca)"
        type: string
        required: false
      hansards_limit:
        description: "Max paragraphs from Hansards (0 = unlimited)"
        type: number
        default: 5000
      strict_qc:
        description: "Strict fr-CA filter"
        type: boolean
        default: true
      ratios:
        description: "Sampling ratios (sum≈1), e.g. frca:0.65,agent:0.25,repo:0.10"
        type: string
        default: "frca:0.65,agent:0.25,repo:0.10"
      val_pct:
        description: "Validation split percentage"
        type: number
        default: 0.06
      retention_days:
        description: "Artifact retention (days)"
        type: number
        default: 14

permissions:
  contents: read

env:
  PYTHON_VERSION: "3.12"

jobs:
  scan:
    name: Scan & Mine Repo
    runs-on: ubuntu-latest
    env:
      CONFIRM_SCAN: "YES"
      CONFIRM_DATA_FETCH: "NO"
    steps:
      - name: Check out
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install system deps (graphviz)
        run: |
          sudo apt-get update
          sudo apt-get install -y graphviz

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install networkx

            - name: Create analyzer script (lite)
        run: |
          mkdir -p scripts
          cat > scripts/ultimate_repo_analyzer.py <<'PY'
#!/usr/bin/env python3
# Lite repo analyzer: mines SFT, agent-handoff, embeddings, and writes a DOT graph.
import os, re, json, csv, hashlib, subprocess
from pathlib import Path
import sys

OUT = Path("data/ultimate")
RAW = OUT / "raw_texts"
PROC = OUT / "processed_repo"
RAW.mkdir(parents=True, exist_ok=True)
PROC.mkdir(parents=True, exist_ok=True)

SFT = PROC / "sft_repo.jsonl"
AGT = PROC / "agent_instruct_repo.jsonl"
EMB = PROC / "embeddings_repo.jsonl"
PROV = PROC / "provenance.csv"
DOT = OUT / "interaction_graph.dot"
PNG = OUT / "interaction_graph.png"

CONFIRM_SCAN = os.environ.get("CONFIRM_SCAN","")
if CONFIRM_SCAN != "YES":
    print("ABORT: set CONFIRM_SCAN=YES to allow local scanning.")
    sys.exit(2)

EXTS = {"md","rst","txt","json","yml","yaml","py","sh","cfg","ini","toml","sql","js","ts"}
def is_text(p: Path) -> bool:
    try:
        with p.open("rb") as f:
            b=f.read(4096)
            return b"\x00" not in b
    except Exception:
        return False

# 1) collect files (respect .gitignore if available)
root = Path(".").resolve()
files=[]
if (root/".git").exists():
    try:
        out = subprocess.check_output(["git","ls-files"], text=True)
        for rel in out.splitlines():
            p = root/rel
            if p.suffix.lstrip(".") in EXTS and p.exists() and is_text(p):
                dst = RAW / p.relative_to(root)
                dst.parent.mkdir(parents=True, exist_ok=True)
                dst.write_bytes(p.read_bytes())
                files.append(dst)
    except Exception:
        pass
if not files:
    for p in root.rglob("*"):
        if p.is_file() and p.suffix.lstrip(".") in EXTS and is_text(p) and ".git" not in p.parts:
            dst = RAW / p.relative_to(root)
            dst.parent.mkdir(parents=True, exist_ok=True)
            dst.write_bytes(p.read_bytes())
            files.append(dst)

# 2) mine SFT, agent, embeddings
DIALOG = re.compile(r'^\s*(User|Utilisateur|Client|Moi|Tu|Vous|Assistant|System|Bot|Agent)\s*[:\-—]\s*(.+)', re.I)
PIPE   = re.compile(r'(workflow|pipeline|job|stage|steps|run:|script:|entrypoint|commands?)', re.I)
JSON_L = re.compile(r'^\s*[\{\[]\s*".*')

def sha(s): return hashlib.sha1(s.encode("utf-8")).hexdigest()[:12]

sft_rows=[]; ag_rows=[]; emb_rows=[]
prov = []

for f in files:
    try:
        txt = f.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        continue
    lines = txt.splitlines()

    # Dialogue → SFT
    cur=[]
    for i,ln in enumerate(lines):
        if DIALOG.match(ln): cur.append((i+1, ln.strip()))
        else:
            if cur:
                instr=None; outs=[]
                for _,l in cur:
                    m=DIALOG.match(l); who=m.group(1).lower(); content=m.group(2).strip()
                    if instr is None and re.match(r'(user|utilisateur|client|moi|tu|vous)', who, re.I):
                        instr=content
                    else:
                        outs.append(content)
                if instr and outs:
                    rec={"instruction":instr,"input":"","output":" ".join(outs)}
                    sft_rows.append(rec); prov.append([sha(json.dumps(rec,ensure_ascii=False)), str(f.relative_to(RAW)), cur[0][0], cur[-1][0], "sft_dialog","auto"])
                cur=[]
    if cur:
        instr=None; outs=[]
        for _,l in cur:
            m=DIALOG.match(l); who=m.group(1).lower(); content=m.group(2).strip()
            if instr is None and re.match(r'(user|utilisateur|client|moi|tu|vous)', who, re.I):
                instr=content
            else:
                outs.append(content)
        if instr and outs:
            rec={"instruction":instr,"input":"","output":" ".join(outs)}
            sft_rows.append(rec); prov.append([sha(json.dumps(rec,ensure_ascii=False)), str(f.relative_to(RAW)), cur[0][0], cur[-1][0], "sft_dialog","auto"])

    # Pipeline → agent
    curp=[]
    for i,ln in enumerate(lines):
        if PIPE.search(ln) or JSON_L.match(ln) or ln.strip().startswith(("steps:", "- run:", "script:")):
            curp.append((i+1, ln))
        else:
            if curp:
                block="\n".join(l for _,l in curp)
                rec={"instruction":"Interpret this pipeline fragment and return structured steps as JSON. Preserve tool names and env vars.","input":block,"output":{"steps":[],"notes":"AUTO"}}
                ag_rows.append(rec); prov.append([sha(json.dumps(rec,ensure_ascii=False)), str(f.relative_to(RAW)), curp[0][0], curp[-1][0], "agent_pipeline","auto"])
                curp=[]
    if curp:
        block="\n".join(l for _,l in curp)
        rec={"instruction":"Interpret this pipeline fragment and return structured steps as JSON. Preserve tool names and env vars.","input":block,"output":{"steps":[],"notes":"AUTO"}}
        ag_rows.append(rec); prov.append([sha(json.dumps(rec,ensure_ascii=False)), str(f.relative_to(RAW)), curp[0][0], curp[-1][0], "agent_pipeline","auto"])

    # Paragraphs → embeddings
    para=[]; start=1
    for i,ln in enumerate(lines):
        if ln.strip()=="":
            if para:
                text="\n".join(para).strip()
                if len(text)>=40:
                    emb_rows.append({"text":text,"source":str(f.relative_to(RAW))})
                    prov.append([sha(text), str(f.relative_to(RAW)), start, i, "embedding_paragraph","auto"])
                para=[]
            start=i+2
        else:
            para.append(ln)
    if para:
        text="\n".join(para).strip()
        if len(text)>=40:
            emb_rows.append({"text":text,"source":str(f.relative_to(RAW))})
            prov.append([sha(text), str(f.relative_to(RAW)), start, len(lines), "embedding_paragraph","auto"])

# write outputs
def write_jsonl(path, rows):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w",encoding="utf-8") as f:
        for r in rows: f.write(json.dumps(r,ensure_ascii=False)+"\n")

write_jsonl(SFT, sft_rows)
write_jsonl(AGT, ag_rows)
write_jsonl(EMB, emb_rows)
with PROV.open("w",encoding="utf-8",newline="") as f:
    w=csv.writer(f); w.writerow(["record_id","source_file","start_line","end_line","type","note"])
    w.writerows(prov)

# minimal DOT graph (imports are skipped in lite version)
with DOT.open("w",encoding="utf-8") as f:
    f.write("digraph interactions {\n")
    f.write('  "repo" [label="repo"];\n')
    for i in range(min(20, len(sft_rows))):
        f.write(f'  "repo" -> "sft_{i}";\n')
    f.write("}\n")

# render PNG if graphviz present
try:
    subprocess.run(["dot","-Tpng", str(DOT), "-o", str(PNG)], check=True)
except Exception:
    pass

print(f"OK: SFT={len(sft_rows)} AGENT={len(ag_rows)} EMB={len(emb_rows)}")
print("Wrote:", SFT, AGT, EMB, PROV, DOT, PNG if PNG.exists() else "")
PY
          chmod +x scripts/ultimate_repo_analyzer.py
      
      - name: Run repo analyzer (scan-only)
        run: |
          python scripts/ultimate_repo_analyzer.py \
            --out data/ultimate \
            --graph \
            --systemd

      - name: Upload scan artifacts
        uses: actions/upload-artifact@v4
        with:
          name: repo-scan-artifacts
          path: |
            data/ultimate/processed_repo/sft_repo.jsonl
            data/ultimate/processed_repo/agent_instruct_repo.jsonl
            data/ultimate/processed_repo/embeddings_repo.jsonl
            data/ultimate/processed_repo/provenance.csv
            data/ultimate/interaction_graph.dot
            data/ultimate/interaction_graph.png
          retention-days: ${{ inputs.retention_days }}
          if-no-files-found: error

  fetch_external:
    name: Fetch External Sources
    runs-on: ubuntu-latest
    if: ${{ inputs.enable_external_fetch == true }}
    env:
      CONFIRM_DATA_FETCH: "YES"
    steps:
      - name: Check out
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install "datasets>=2.0.0,<3.0.0" beautifulsoup4 requests tldextract

      - name: Fetch Hansards fr-CA (usc-isi/hansards)
        run: |
          python - <<'PY'
          import re, json
          from pathlib import Path
          from datasets import load_dataset

          limit = int("${{ inputs.hansards_limit }}")
          out = Path("data/external/frca_hansard.jsonl")
          out.parent.mkdir(parents=True, exist_ok=True)

          ds = load_dataset("usc-isi/hansards", split="train")
          rows, count = [], 0
          for x in ds:
              fr = (x.get("french") or "").strip()
              if not fr: continue
              for para in re.split(r"\n\s*\n", fr):
                  para = para.strip()
                  if len(para) < 50: continue
                  rows.append({"instruction": para, "input": "", "output": para})
                  count += 1
                  if limit and count >= limit: break
              if limit and count >= limit: break

          with out.open("w", encoding="utf-8") as f:
              for r in rows:
                  f.write(json.dumps(r, ensure_ascii=False) + "\n")
          print(f"Wrote {len(rows)} → {out}")
          PY

      - name: Crawl allow-listed domains (optional)
        if: ${{ inputs.crawl_domains != '' }}
        run: |
          python - <<'PY'
          import json, requests, tldextract
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin, urlparse
          from pathlib import Path

          domains = [d.strip() for d in """${{ inputs.crawl_domains }}""".splitlines() if d.strip()]
          max_pages, timeout = 200, 8
          out = Path("data/external/frca_crawl.jsonl")
          out.parent.mkdir(parents=True, exist_ok=True)
          rows = []

          def can_fetch(base):
              try:
                  rp = requests.get(urljoin(base, "/robots.txt"), timeout=timeout)
                  return not (rp.status_code == 200 and "disallow: /" in rp.text.lower())
              except Exception:
                  return True

          visited, queue = set(), []
          for d in domains:
              base = d if d.startswith("http") else "https://" + d
              if can_fetch(base): queue.append(base)

          while queue and len(visited) < max_pages:
              url = queue.pop(0)
              if url in visited: continue
              visited.add(url)
              try:
                  r = requests.get(url, timeout=timeout, headers={"User-Agent":"monGARS-safe-crawler"})
                  if r.status_code != 200 or "text/html" not in r.headers.get("Content-Type",""): continue
                  soup = BeautifulSoup(r.text, "html.parser")
                  for t in [x.get_text(" ", strip=True) for x in soup.find_all(["p","li","blockquote"])]:
                      t = t.strip()
                      if len(t) >= 50:
                          rows.append({"instruction": t, "input": "", "output": t})
                  base_reg = tldextract.extract(url).registered_domain
                  for a in soup.find_all("a", href=True):
                      nu = urljoin(url, a["href"])
                      pr = urlparse(nu)
                      if pr.scheme in ("http","https"):
                          reg = tldextract.extract(nu).registered_domain
                          if reg == base_reg and nu not in visited and len(queue) < max_pages:
                              queue.append(nu)
              except Exception:
                  pass

          with out.open("w", encoding="utf-8") as f:
              for r in rows:
                  f.write(json.dumps(r, ensure_ascii=False) + "\n")
          print(f"Crawled {len(visited)} pages; wrote {len(rows)} → {out}")
          PY

      - name: Upload external JSONLs
        uses: actions/upload-artifact@v4
        with:
          name: external-sources
          path: |
            data/external/frca_hansard.jsonl
            data/external/frca_crawl.jsonl
          retention-days: ${{ inputs.retention_days }}
          if-no-files-found: warn

  build_dataset:
    name: Build Final Dataset
    runs-on: ubuntu-latest
    needs: [scan]   # external job is optional
    steps:
      - name: Check out
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip

      - name: Download repo scan artifacts
        uses: actions/download-artifact@v4
        with:
          name: repo-scan-artifacts
          path: repo-scan-artifacts

      - name: Download external sources (if any)
        if: ${{ inputs.enable_external_fetch == true }}
        uses: actions/download-artifact@v4
        with:
          name: external-sources
          path: external-sources

      - name: Assemble inputs
        run: |
          mkdir -p data/raw
          cp repo-scan-artifacts/sft_repo.jsonl            data/raw/repo_sft.jsonl
          cp repo-scan-artifacts/agent_instruct_repo.jsonl data/raw/agent_handoff.jsonl
          if [ "${{ inputs.enable_external_fetch }}" = "true" ] && [ -f external-sources/frca_hansard.jsonl ]; then
            cp external-sources/frca_hansard.jsonl data/raw/frca_hansard.jsonl
            echo "FRCA=data/raw/frca_hansard.jsonl" >> $GITHUB_ENV
          else
            echo "FRCA=data/raw/repo_sft.jsonl" >> $GITHUB_ENV
          fi

      - name: Build final dataset (train/val)
        run: |
          python scripts/prepare_dataset.py \
            --frca "$FRCA" \
            --agent data/raw/agent_handoff.jsonl \
            --repo data/raw/repo_sft.jsonl \
            --outdir data/final \
            --ratio "${{ inputs.ratios }}" \
            --val_pct ${{ inputs.val_pct }} \
            $([ "${{ inputs.strict_qc }}" = "true" ] && echo "--strict_qc" || true)

      - name: Upload final dataset
        uses: actions/upload-artifact@v4
        with:
          name: final-custom-dataset
          path: |
            data/final/train.jsonl
            data/final/val.jsonl
          retention-days: ${{ inputs.retention_days }}
          if-no-files-found: error

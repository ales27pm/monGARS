name: Build Custom Québec-French Dataset

on:
  push:
    branches: ['**']
  workflow_dispatch:
    inputs:
      ratios:
        description: "Sampling ratios (sum≈1), e.g. frca:0.50,agent:0.40,repo:0.10"
        type: string
        default: "frca:0.50,agent:0.40,repo:0.10"
      val_pct:
        description: "Validation split percentage"
        type: number
        default: 0.06
      strict_qc:
        description: "Strict fr-CA filter (drop non-Québécois unless agent/internal)"
        type: boolean
        default: true
      retention_days:
        description: "Artifact retention (days)"
        type: number
        default: 14

permissions:
  contents: read

env:
  PYTHON_VERSION: "3.12"
  SCAN_OUTPUT_DIR: data/deep_scan
  FINAL_OUTPUT_DIR: data/final
  DATASET_RATIOS: ${{ github.event.inputs.ratios || 'frca:0.50,agent:0.40,repo:0.10' }}
  DATASET_VAL_PCT: ${{ github.event.inputs.val_pct || 0.06 }}
  DATASET_STRICT_QC: ${{ github.event.inputs.strict_qc || 'true' }}
  DATASET_RETENTION_DAYS: ${{ github.event.inputs.retention_days || 14 }}

concurrency:
  group: custom-dataset-${{ github.ref }}
  cancel-in-progress: true

jobs:
  curate:
    name: Curate Québec-French dataset
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip downloads
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-${{ hashFiles('tools/monGARS_deep_scan/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-
            ${{ runner.os }}-pip-

      - name: Install dataset tooling
        run: |
          python -m pip install --upgrade pip
          pip install -r tools/monGARS_deep_scan/requirements.txt

      - name: Execute deep scan pipeline
        env:
          PYTHONPATH: .
        run: |
          set -euxo pipefail
          python -m tools.monGARS_deep_scan.deep_scan \
            --input . \
            --out "${SCAN_OUTPUT_DIR}" \
            --exclude-dir "${SCAN_OUTPUT_DIR},${FINAL_OUTPUT_DIR},.git" \
            --jobs 4
          ls "${SCAN_OUTPUT_DIR}"

      - name: Build train/validation splits
        env:
          PYTHONPATH: .
        run: |
          python - <<'PY'
import datetime as dt
import json
import os
import random
import sys
from pathlib import Path

from tools.monGARS_deep_scan.utils.hashing import stable_hash

scan_dir = Path(os.environ["SCAN_OUTPUT_DIR"]).resolve()
final_dir = Path(os.environ["FINAL_OUTPUT_DIR"]).resolve()
ratios_raw = os.environ.get("DATASET_RATIOS", "frca:0.50,agent:0.40,repo:0.10")
val_pct = float(os.environ.get("DATASET_VAL_PCT", 0.06))
strict_qc = str(os.environ.get("DATASET_STRICT_QC", "true")).lower() not in {"0", "false", "no"}
seed_value = os.environ.get("DATASET_SEED") or os.environ.get("GITHUB_RUN_ID") or "42"
try:
    random.seed(int(seed_value))
except ValueError:
    random.seed(seed_value)

if not scan_dir.exists():
    raise SystemExit(f"Scan directory {scan_dir} not found")
final_dir.mkdir(parents=True, exist_ok=True)

ratios: dict[str, float] = {}
for part in ratios_raw.split(','):
    part = part.strip()
    if not part:
        continue
    if ':' not in part:
        print(f"::warning::Ignoring malformed ratio entry '{part}'", file=sys.stderr)
        continue
    key, value = part.split(':', 1)
    try:
        ratios[key.strip()] = float(value)
    except ValueError:
        print(f"::warning::Invalid ratio value '{value}' for key '{key.strip()}'", file=sys.stderr)

if not ratios:
    ratios = {"frca": 0.5, "agent": 0.4, "repo": 0.1}
ratio_total = sum(ratios.values())
if ratio_total > 0:
    ratios = {key: value / ratio_total for key, value in ratios.items()}
for bucket_name in ("frca", "agent", "repo"):
    ratios.setdefault(bucket_name, 0.0)

sft_path = scan_dir / "sft_dataset.jsonl"
agent_path = scan_dir / "agent_handoff_dataset.jsonl"
if not sft_path.exists() or not agent_path.exists():
    raise SystemExit("Deep scan outputs missing required dataset files")


def load_jsonl(path: Path):
    with path.open("r", encoding="utf-8") as handle:
        for line_no, line in enumerate(handle, start=1):
            line = line.strip()
            if not line:
                continue
            try:
                yield json.loads(line)
            except json.JSONDecodeError as exc:
                print(f"::warning::Skipping invalid JSON in {path} line {line_no}: {exc}", file=sys.stderr)


MIN_TEXT = 12
MAX_OUTPUT_CHARS = 3000


def normalise(record: dict) -> dict | None:
    instruction = (record.get("instruction") or "").strip()
    input_text = (record.get("input") or "").strip()
    output = record.get("output")
    if isinstance(output, (dict, list)):
        output = json.dumps(output, ensure_ascii=False, separators=(",", ":"))
    elif output is None:
        output = ""
    else:
        output = str(output).strip()
    if len(instruction) < MIN_TEXT or len(output) < MIN_TEXT:
        return None
    if len(output) > MAX_OUTPUT_CHARS:
        output = output[:MAX_OUTPUT_CHARS].rsplit(" ", 1)[0] + " …"
    return {"instruction": instruction, "input": input_text, "output": output}


sft_records: list[tuple[dict, bool]] = []
for record in load_jsonl(sft_path):
    parsed = normalise(record)
    if not parsed:
        continue
    qc_flag = bool(record.get("_meta", {}).get("qc_fr_ca", False))
    sft_records.append((parsed, qc_flag))

buckets: dict[str, list[dict]] = {"frca": [], "agent": [], "repo": []}
for parsed, qc_flag in sft_records:
    if qc_flag or not strict_qc:
        buckets["frca"].append(parsed)
    if qc_flag or not strict_qc:
        buckets["repo"].append(parsed)

for record in load_jsonl(agent_path):
    parsed = normalise(record)
    if not parsed:
        continue
    buckets["agent"].append(parsed)

# Deduplicate within each bucket
for key, records in buckets.items():
    seen: set[str] = set()
    unique: list[dict] = []
    for row in records:
        record_hash = stable_hash([
            row.get("instruction", ""),
            row.get("input", ""),
            row.get("output", ""),
        ])
        if record_hash in seen:
            continue
        seen.add(record_hash)
        unique.append(row)
    buckets[key] = unique

all_records: list[dict] = []
record_sources: dict[str, set[str]] = {}
for records in buckets.values():
    all_records.extend(records)

for bucket_name, records in buckets.items():
    for row in records:
        record_id = stable_hash([
            row.get("instruction", ""),
            row.get("input", ""),
            row.get("output", ""),
        ])
        record_sources.setdefault(record_id, set()).add(bucket_name)

if not all_records:
    raise SystemExit("No qualifying records available for dataset assembly")

unique_total = len({
    stable_hash([
        row.get("instruction", ""),
        row.get("input", ""),
        row.get("output", ""),
    ])
    for row in all_records
})

source_counts = {key: len(value) for key, value in buckets.items()}
requested_total = max(unique_total, 1)

mixed: list[dict] = []
selected_ids: set[str] = set()
selected_breakdown: dict[str, int] = {"frca": 0, "agent": 0, "repo": 0}

for key in ("frca", "agent", "repo"):
    records = buckets.get(key, [])
    if not records:
        continue
    random.shuffle(records)
    target = int(round(ratios.get(key, 0.0) * requested_total))
    if ratios.get(key, 0.0) > 0 and target == 0:
        target = 1
    target = min(target, len(records))
    taken = 0
    for row in records:
        record_id = stable_hash([
            row.get("instruction", ""),
            row.get("input", ""),
            row.get("output", ""),
        ])
        if record_id in selected_ids:
            continue
        mixed.append(row)
        selected_ids.add(record_id)
        selected_breakdown[key] = selected_breakdown.get(key, 0) + 1
        taken += 1
        if taken >= target:
            break

if len(mixed) < min(unique_total, requested_total):
    pool = all_records.copy()
    random.shuffle(pool)
    for row in pool:
        if len(mixed) >= min(unique_total, requested_total):
            break
        record_id = stable_hash([
            row.get("instruction", ""),
            row.get("input", ""),
            row.get("output", ""),
        ])
        if record_id in selected_ids:
            continue
        mixed.append(row)
        selected_ids.add(record_id)
        source_candidates = sorted(record_sources.get(record_id, {"repo"}))
        origin = source_candidates[0] if source_candidates else "repo"
        selected_breakdown[origin] = selected_breakdown.get(origin, 0) + 1

random.shuffle(mixed)

val_count = int(len(mixed) * val_pct)
validation = mixed[:val_count]
training = mixed[val_count:]

train_path = final_dir / "train.jsonl"
val_path = final_dir / "val.jsonl"

for path, dataset in ((train_path, training), (val_path, validation)):
    with path.open("w", encoding="utf-8") as handle:
        for row in dataset:
            handle.write(json.dumps(row, ensure_ascii=False) + "\n")

summary = {
    "generated_at": dt.datetime.utcnow().isoformat(timespec="seconds") + "Z",
    "commit": os.environ.get("GITHUB_SHA"),
    "requested_ratios": ratios,
    "strict_qc": strict_qc,
    "validation_fraction": val_pct,
    "train_records": len(training),
    "validation_records": len(validation),
    "total_records": len(mixed),
    "source_counts": source_counts,
    "selected_counts": selected_breakdown,
    "actual_ratios": {
        key: (selected_breakdown.get(key, 0) / len(mixed) if mixed else 0.0)
        for key in sorted(selected_breakdown)
    },
}

summary_path = final_dir / "dataset_summary.json"
summary_path.write_text(json.dumps(summary, indent=2, ensure_ascii=False), encoding="utf-8")
print(json.dumps(summary, indent=2, ensure_ascii=False))
PY

      - name: Publish dataset summary
        run: |
          python - <<'PY'
import json
from pathlib import Path

summary_path = Path("${FINAL_OUTPUT_DIR}") / "dataset_summary.json"
if not summary_path.exists():
    raise SystemExit("Dataset summary not found")
summary = json.loads(summary_path.read_text(encoding="utf-8"))
lines = [
    "# Québec-French dataset build",
    "",
    f"* Total records: {summary['total_records']}",
    f"* Train records: {summary['train_records']}",
    f"* Validation records: {summary['validation_records']}",
    f"* Strict QC: {summary['strict_qc']}",
    "",
    "## Source buckets",
]
for key, value in summary.get("source_counts", {}).items():
    lines.append(f"- {key}: {value}")
lines.append("")
lines.append("## Requested ratios")
for key, value in summary.get("requested_ratios", {}).items():
    lines.append(f"- {key}: {value}")
lines.append("")
lines.append("## Selected counts")
for key, value in summary.get("selected_counts", {}).items():
    lines.append(f"- {key}: {value}")
lines.append("")
lines.append("## Actual ratios")
for key, value in summary.get("actual_ratios", {}).items():
    lines.append(f"- {key}: {value:.4f}")

summary_md = "\n".join(lines)
with open("summary.md", "w", encoding="utf-8") as handle:
    handle.write(summary_md)
print(summary_md)
PY
          cat summary.md >> "$GITHUB_STEP_SUMMARY"

      - name: Upload dataset artefacts
        uses: actions/upload-artifact@v4
        with:
          name: custom-quebec-french-dataset
          path: |
            ${SCAN_OUTPUT_DIR}/sft_dataset.jsonl
            ${SCAN_OUTPUT_DIR}/agent_handoff_dataset.jsonl
            ${SCAN_OUTPUT_DIR}/embeddings_corpus.jsonl
            ${SCAN_OUTPUT_DIR}/provenance.csv
            ${SCAN_OUTPUT_DIR}/report.md
            ${SCAN_OUTPUT_DIR}/logs/**
            ${FINAL_OUTPUT_DIR}/train.jsonl
            ${FINAL_OUTPUT_DIR}/val.jsonl
            ${FINAL_OUTPUT_DIR}/dataset_summary.json
          retention-days: ${{ env.DATASET_RETENTION_DAYS }}
          if-no-files-found: error

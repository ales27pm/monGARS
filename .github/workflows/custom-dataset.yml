name: Build Custom Québec-French Dataset (Repo-only)

on:
  workflow_dispatch:
    inputs:
      ratios:
        description: "Sampling ratios (sum≈1), e.g. frca:0.65,agent:0.25,repo:0.10"
        type: string
        default: "frca:0.50,agent:0.40,repo:0.10"
      val_pct:
        description: "Validation split percentage"
        type: number
        default: 0.06
      strict_qc:
        description: "Strict fr-CA filter (drop non-Québécois unless agent/internal)"
        type: boolean
        default: true
      retention_days:
        description: "Artifact retention (days)"
        type: number
        default: 14

permissions:
  contents: read

env:
  PYTHON_VERSION: "3.12"

jobs:
  scan_and_build:
    runs-on: ubuntu-latest
    env:
      CONFIRM_SCAN: "YES"
      CONFIRM_DATA_FETCH: "NO"
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install OS deps
        run: |
          sudo apt-get update
          sudo apt-get install -y graphviz

      - name: Create analyzer script (lite)
        run: |
          set -euo pipefail
          mkdir -p scripts
          cat > scripts/ultimate_repo_analyzer.py <<'PY'
#!/usr/bin/env python3
import os, re, json, csv, hashlib, subprocess, sys
from pathlib import Path

OUT = Path("data/ultimate"); RAW = OUT/"raw_texts"; PROC = OUT/"processed_repo"
for d in (RAW, PROC): d.mkdir(parents=True, exist_ok=True)
SFT = PROC/"sft_repo.jsonl"; AGT = PROC/"agent_instruct_repo.jsonl"
EMB = PROC/"embeddings_repo.jsonl"; PROV = PROC/"provenance.csv"
DOT = OUT/"interaction_graph.dot"; PNG = OUT/"interaction_graph.png"

if os.environ.get("CONFIRM_SCAN","") != "YES":
    print("ABORT: set CONFIRM_SCAN=YES"); sys.exit(2)

EXTS = {"md","rst","txt","json","yml","yaml","py","sh","cfg","ini","toml","sql","js","ts"}
def is_text(p): 
    try:
        with p.open("rb") as f: return b"\x00" not in f.read(4096)
    except: return False

root = Path(".").resolve(); files=[]
if (root/".git").exists():
    try:
        out = subprocess.check_output(["git","ls-files"], text=True)
        for rel in out.splitlines():
            p=root/rel
            if p.suffix.lstrip(".") in EXTS and p.exists() and is_text(p):
                dst=RAW/p.relative_to(root); dst.parent.mkdir(parents=True, exist_ok=True)
                dst.write_bytes(p.read_bytes()); files.append(dst)
    except: pass
if not files:
    for p in root.rglob("*"):
        if p.is_file() and p.suffix.lstrip(".") in EXTS and is_text(p) and ".git" not in p.parts:
            dst=RAW/p.relative_to(root); dst.parent.mkdir(parents=True, exist_ok=True)
            dst.write_bytes(p.read_bytes()); files.append(dst)

DIALOG=re.compile(r'^\s*(User|Utilisateur|Client|Moi|Tu|Vous|Assistant|System|Bot|Agent)\s*[:\-—]\s*(.+)', re.I)
PIPE=re.compile(r'(workflow|pipeline|job|stage|steps|run:|script:|entrypoint|commands?)', re.I)
JSONL=re.compile(r'^\s*[\{\[]\s*".*')
def sha(s): import hashlib; return hashlib.sha1(s.encode("utf-8")).hexdigest()[:12]
sft_rows=[]; ag_rows=[]; emb_rows=[]; prov=[]

for f in files:
    try: text=f.read_text(encoding="utf-8", errors="ignore")
    except: continue
    lines=text.splitlines()

    cur=[]
    for i,ln in enumerate(lines):
        if DIALOG.match(ln): cur.append((i+1, ln.strip()))
        else:
            if cur:
                instr=None; outs=[]
                for _,l in cur:
                    m=DIALOG.match(l); who=m.group(1).lower(); content=m.group(2).strip()
                    if instr is None and re.match(r'(user|utilisateur|client|moi|tu|vous)', who, re.I): instr=content
                    else: outs.append(content)
                if instr and outs:
                    rec={"instruction":instr,"input":"","output":" ".join(outs)}
                    sft_rows.append(rec); prov.append([sha(json.dumps(rec,ensure_ascii=False)), str(f.relative_to(RAW)), cur[0][0], cur[-1][0], "sft_dialog","auto"])
                cur=[]
    if cur:
        instr=None; outs=[]
        for _,l in cur:
            m=DIALOG.match(l); who=m.group(1).lower(); content=m.group(2).strip()
            if instr is None and re.match(r'(user|utilisateur|client|moi|tu|vous)', who, re.I): instr=content
            else: outs.append(content)
        if instr and outs:
            rec={"instruction":instr,"input":"","output":" ".join(outs)}
            sft_rows.append(rec); prov.append([sha(json.dumps(rec,ensure_ascii=False)), str(f.relative_to(RAW)), cur[0][0], cur[-1][0], "sft_dialog","auto"])

    curp=[]
    for i,ln in enumerate(lines):
        if PIPE.search(ln) or JSONL.match(ln) or ln.strip().startswith(("steps:", "- run:", "script:")):
            curp.append((i+1, ln))
        else:
            if curp:
                block="\n".join(l for _,l in curp)
                rec={"instruction":"Interpret this pipeline fragment and return structured steps as JSON. Preserve tool names and env vars.","input":block,"output":{"steps":[],"notes":"AUTO"}}
                ag_rows.append(rec); prov.append([sha(json.dumps(rec,ensure_ascii=False)), str(f.relative_to(RAW)), curp[0][0], curp[-1][0], "agent_pipeline","auto"])
                curp=[]
    if curp:
        block="\n".join(l for _,l in curp)
        rec={"instruction":"Interpret this pipeline fragment and return structured steps as JSON. Preserve tool names and env vars.","input":block,"output":{"steps":[],"notes":"AUTO"}}
        ag_rows.append(rec); prov.append([sha(json.dumps(rec,ensure_ascii=False)), str(f.relative_to(RAW)), curp[0][0], curp[-1][0], "agent_pipeline","auto"])

    para=[]; start=1
    for i,ln in enumerate(lines):
        if ln.strip()=="":
            if para:
                t="\n".join(para).strip()
                if len(t)>=40:
                    emb_rows.append({"text":t,"source":str(f.relative_to(RAW))})
                    prov.append([sha(t), str(f.relative_to(RAW)), start, i, "embedding_paragraph","auto"])
                para=[]; start=i+2
        else:
            para.append(ln)
    if para:
        t="\n".join(para).strip()
        if len(t)>=40:
            emb_rows.append({"text":t,"source":str(f.relative_to(RAW))})
            prov.append([sha(t), str(f.relative_to(RAW)), start, len(lines), "embedding_paragraph","auto"])

def write_jsonl(p, rows):
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("w", encoding="utf-8") as f:
        for r in rows: f.write(json.dumps(r, ensure_ascii=False)+"\n")

write_jsonl(SFT, sft_rows); write_jsonl(AGT, ag_rows); write_jsonl(EMB, emb_rows)
with PROV.open("w", encoding="utf-8", newline="") as f:
    w=csv.writer(f); w.writerow(["record_id","source_file","start_line","end_line","type","note"]); w.writerows(prov)

with DOT.open("w", encoding="utf-8") as f:
    f.write("digraph interactions {\n  \"repo\" [label=\"repo\"];\n")
    for i in range(min(20, len(sft_rows))): f.write(f'  "repo" -> "sft_{i}";\n')
    f.write("}\n")
try: subprocess.run(["dot","-Tpng", str(DOT), "-o", str(PNG)], check=True)
except: pass

print(f"OK: SFT={len(sft_rows)} AGENT={len(ag_rows)} EMB={len(emb_rows)}")
print("OUT:", SFT, AGT, EMB, PROV, DOT, PNG if PNG.exists() else "(no PNG)")
PY
          chmod +x scripts/ultimate_repo_analyzer.py

      - name: Run analyzer
        run: |
          set -euxo pipefail
          python scripts/ultimate_repo_analyzer.py
          echo "== Output tree =="
          find data/ultimate -maxdepth 3 -type f | sort || true

      - name: Create dataset builder
        run: |
          set -euo pipefail
          mkdir -p scripts
          cat > scripts/prepare_dataset.py <<'PY'
#!/usr/bin/env python3
import json, random, argparse, hashlib
from pathlib import Path
QC_TERMS=["dépanneur","poutine","cégep","tuque","magasiner","char","chum","blonde","icitte","ben là","patente","tabarnak"]
MIN_LEN=12; MAX_OUT_CHARS=3000
def sha(s): import hashlib; return hashlib.sha1(s.encode("utf-8")).hexdigest()
def is_qc(t): t=t.lower(); return any(w in t for w in QC_TERMS)
def clamp(s,n): s=s.strip(); return s if len(s)<=n else s[:n].rsplit(" ",1)[0]+" …"
def load_jsonl(p): 
    if not Path(p).exists(): return []
    with open(p,"r",encoding="utf-8") as f:
        for line in f:
            line=line.strip()
            if line: yield json.loads(line)
def norm_sft(j):
    if not isinstance(j,dict) or "instruction" not in j or "output" not in j: return None
    instr=(j.get("instruction") or "").strip(); inp=(j.get("input") or "").strip(); out=j.get("output")
    if not isinstance(out,str): out=json.dumps(out,ensure_ascii=False,separators=(",",":"))
    out=clamp(out,MAX_OUT_CHARS)
    if len(instr)<MIN_LEN or len(out)<MIN_LEN: return None
    return {"instruction":instr,"input":inp,"output":out}
def dedupe(xs,key=lambda x:x):
    s=set(); o=[]
    for it in xs:
        k=key(it)
        if k in s: continue
        s.add(k); o.append(it)
    return o
def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--frca", default="data/raw/repo_sft.jsonl") # repo-only default
    ap.add_argument("--agent", default="data/raw/agent_handoff.jsonl")
    ap.add_argument("--repo", default="data/raw/repo_sft.jsonl")
    ap.add_argument("--outdir", default="data/final")
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--ratio", default="frca:0.50,agent:0.40,repo:0.10")
    ap.add_argument("--val_pct", type=float, default=0.06)
    ap.add_argument("--strict_qc", action="store_true")
    a=ap.parse_args(); random.seed(a.seed)
    ratios={}; 
    for part in a.ratio.split(","):
        k,v=part.split(":"); ratios[k.strip()]=float(v)
    sources={"frca":a.frca,"agent":a.agent,"repo":a.repo}
    buckets={}
    for name, path in sources.items():
        rows=[]
        for j in load_jsonl(path):
            s=norm_sft(j)
            if not s: continue
            if a.strict_qc and name!="agent":
                if not is_qc(s["instruction"]+" "+s["output"]): continue
            rows.append(s)
        rows=dedupe(rows, key=lambda x: sha((x["instruction"]+"|"+x["input"]).lower()))
        buckets[name]=rows
        print(f"[LOAD] {name}: {len(rows)}")
    total=sum(len(v) for v in buckets.values())
    if total==0: raise SystemExit("No data")
    targets={k:int(ratios.get(k,0)*total) for k in buckets}
    mixed=[]
    for k, arr in buckets.items():
        random.shuffle(arr)
        take=min(len(arr), max(0, targets.get(k,0)))
        mixed.extend(arr[:take])
    if len(mixed)<total:
        pool=[x for xs in buckets.values() for x in xs]; random.shuffle(pool)
        for x in pool:
            if len(mixed)>=total: break
            mixed.append(x)
    random.shuffle(mixed)
    n_val=int(len(mixed)*a.val_pct); val=mixed[:n_val]; train=mixed[n_val:]
    out=Path(a.outdir); out.mkdir(parents=True,exist_ok=True)
    with (out/"train.jsonl").open("w",encoding="utf-8") as f:
        for r in train: f.write(json.dumps(r,ensure_ascii=False)+"\n")
    with (out/"val.jsonl").open("w",encoding="utf-8") as f:
        for r in val: f.write(json.dumps(r,ensure_ascii=False)+"\n")
    print(f"[DONE] train={len(train)} val={len(val)} strict_qc={a.strict_qc}")
if __name__=="__main__": main()
PY
          chmod +x scripts/prepare_dataset.py

      - name: Assemble inputs
        run: |
          set -euxo pipefail
          mkdir -p data/raw
          # analyzer outputs -> expected inputs
          cp data/ultimate/processed_repo/sft_repo.jsonl            data/raw/repo_sft.jsonl
          cp data/ultimate/processed_repo/agent_instruct_repo.jsonl data/raw/agent_handoff.jsonl
          echo "== Assembled data/raw =="
          ls -l data/raw || true

      - name: Build final dataset
        run: |
          set -euxo pipefail
          python scripts/prepare_dataset.py \
            --frca data/raw/repo_sft.jsonl \
            --agent data/raw/agent_handoff.jsonl \
            --repo data/raw/repo_sft.jsonl \
            --outdir data/final \
            --ratio "${{ inputs.ratios }}" \
            --val_pct ${{ inputs.val_pct }} \
            $([ "${{ inputs.strict_qc }}" = "true" ] && echo "--strict_qc" || true)
          echo "== Final counts =="
          wc -l data/final/train.jsonl || true
          wc -l data/final/val.jsonl   || true

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: final-custom-dataset
          path: |
            data/ultimate/processed_repo/sft_repo.jsonl
            data/ultimate/processed_repo/agent_instruct_repo.jsonl
            data/ultimate/processed_repo/embeddings_repo.jsonl
            data/ultimate/processed_repo/provenance.csv
            data/ultimate/interaction_graph.dot
            data/ultimate/interaction_graph.png
            data/final/train.jsonl
            data/final/val.jsonl
          retention-days: ${{ inputs.retention_days }}
          if-no-files-found: error

{"file": "modules/evolution_engine/orchestrator.py", "line": 72, "function": null, "signature": null, "prompt": "You are a senior engineer. Implement the missing logic near the specified line.\n\nConstraints:\n- Keep behaviour backward-compatible unless tests specify otherwise.\n- Use clear types, docstrings, and precise exceptions.\n- Add minimal unit tests if behaviour isn't covered.\n\nContext (numbered):\n47|     ApprovalPolicy,\n48|     OperatorApprovalRegistry,\n49| )\n50| from monGARS.core.self_training import SelfTrainingEngine\n51| \n52| logger = logging.getLogger(__name__)\n53| \n54| DEFAULT_MODEL_ID = \"dphn/Dolphin3.0-Llama3.1-8B\"\n55| DEFAULT_REGISTRY_PATH = Path(\"models/encoders\")\n56| DEFAULT_CONFIG_PATH = Path(\"configs/training/mntp_dolphin_config.json\")\n57| TRAINING_SUMMARY_FILENAME = \"training_summary.json\"\n58| ENERGY_REPORT_FILENAME = \"energy_report.json\"\n59| TRAINING_SLOT_NAME = \"primary\"\n60| MAX_VRAM_GB = 6.0\n61| CPU_IDLE_THRESHOLD = 20.0\n62| MEMORY_IDLE_THRESHOLD = 70.0\n63| WORKFLOW_NAME = \"evolution-training-flow\"\n64| WORKFLOW_DEPLOYMENT_NAME = \"evolution-training-deployment\"\n65| \n66| CuratedDataset = Sequence[dict[str, Any]] | Any\n67| EnergyTrackerFactory = Callable[[], EnergyTracker]\n68| \n69| \n70| class WorkflowBackend(Protocol):\n71|     \"\"\"Protocol describing the minimal workflow backend surface area.\"\"\"\n72| \n73|     def build_flow(self, func: Callable[..., Any], *, name: str) -> Callable[..., Any]:\n74|         \"\"\"Return a callable representing the orchestrated flow.\"\"\"\n75| \n76|     def ensure_schedule(\n77|         self, flow: Callable[..., Any], *, parameters: Mapping[str, Any]\n78|     ) -> None:\n79|         \"\"\"Register or update the recurring schedule for ``flow``.\"\"\"\n80| \n81|     def run(self, flow: Callable[..., Any], *, parameters: Mapping[str, Any]) -> Any:\n82|         \"\"\"Execute ``flow`` with ``parameters`` and return its result.\"\"\"\n83| \n84| \n85| class InlineWorkflowBackend:\n86|     \"\"\"A lightweight backend that executes flows synchronously.\"\"\"\n87| \n88|     def __init__(self) -> None:\n89|         self.last_schedule: dict[str, Any] | None = None\n90| \n91|     def build_flow(\n92|         self, func: Callable[..., Any], *, name: str\n93|     ) -> Callable[..., Any]:  # noqa: D401 - signature enforced by protocol\n94|         return func\n95| \n96|     def ensure_schedule(\n97|         self, flow: Callable[..., Any], *, parameters: Mapping[str, Any]\n\nDeliverables:\n- Updated code with a complete implementation.\n- Brief note explaining rationale and complexity assumptions.\n", "title": "Implement missing logic near L72 in modules/evolution_engine/orchestrator.py"}
{"file": "scripts/export_llm2vec_wrapper.py", "line": 72, "function": null, "signature": null, "prompt": "You are a senior engineer. Implement the missing logic near the specified line.\n\nConstraints:\n- Keep behaviour backward-compatible unless tests specify otherwise.\n- Use clear types, docstrings, and precise exceptions.\n- Add minimal unit tests if behaviour isn't covered.\n\nContext (numbered):\n47|             model_dir = self.base_dir / \"merged\"\n48|             self.model = AutoModelForCausalLM.from_pretrained(\n49|                 str(model_dir),\n50|                 torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n51|                 device_map=\"auto\",\n52|             )\n53|         else:\n54|             adapter_dir = self.base_dir / \"lora_adapter\"\n55|             if not adapter_dir.exists():\n56|                 raise FileNotFoundError(\"LoRA adapter not found; run training before exporting the wrapper.\")\n57|             base_model = \"dphn/Dolphin3.0-Llama3.1-8B\"\n58|             self.model = AutoModelForCausalLM.from_pretrained(\n59|                 base_model,\n60|                 load_in_4bit=load_in_4bit,\n61|                 device_map=\"auto\",\n62|                 trust_remote_code=True,\n63|             )\n64|             if PeftModel is None:\n65|                 raise RuntimeError(\"peft is required to load the LoRA adapter\")\n66|             self.model = PeftModel.from_pretrained(self.model, str(adapter_dir))\n67| \n68|         self.model.to(self.device)\n69|         self.model.eval()\n70| \n71|     @torch.inference_mode()\n72|     def generate(\n73|         self,\n74|         prompt: str,\n75|         *,\n76|         max_new_tokens: int = 512,\n77|         temperature: float = 0.2,\n78|         top_p: float = 0.9,\n79|     ) -> str:\n80|         inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n81|         outputs = self.model.generate(\n82|             **inputs,\n83|             do_sample=temperature > 0,\n84|             temperature=temperature,\n85|             top_p=top_p,\n86|             max_new_tokens=max_new_tokens,\n87|             pad_token_id=self.tokenizer.eos_token_id,\n88|             eos_token_id=self.tokenizer.eos_token_id,\n89|         )\n90|         return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n91| \n92|     @torch.inference_mode()\n93|     def embed(self, texts: str | Iterable[str]):\n94|         if isinstance(texts, str):\n95|             batch_texts = [texts]\n96|         else:\n97|             batch_texts = list(texts)\n\nDeliverables:\n- Updated code with a complete implementation.\n- Brief note explaining rationale and complexity assumptions.\n", "title": "Implement missing logic near L72 in scripts/export_llm2vec_wrapper.py"}
{"file": "scripts/export_llm2vec_wrapper.py", "line": 147, "function": "LLM2Vec.embed", "signature": "def embed(self, texts: str | Iterable[str]):", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"LLM2Vec.embed\" in file \"scripts/export_llm2vec_wrapper.py\".\n\nSignature:\ndef embed(self, texts: str | Iterable[str]):\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n 53|         else:\n 54|             adapter_dir = self.base_dir / \"lora_adapter\"\n 55|             if not adapter_dir.exists():\n 56|                 raise FileNotFoundError(\"LoRA adapter not found; run training before exporting the wrapper.\")\n 57|             base_model = \"dphn/Dolphin3.0-Llama3.1-8B\"\n 58|             self.model = AutoModelForCausalLM.from_pretrained(\n 59|                 base_model,\n 60|                 load_in_4bit=load_in_4bit,\n 61|                 device_map=\"auto\",\n 62|                 trust_remote_code=True,\n 63|             )\n 64|             if PeftModel is None:\n 65|                 raise RuntimeError(\"peft is required to load the LoRA adapter\")\n 66|             self.model = PeftModel.from_pretrained(self.model, str(adapter_dir))\n 67| \n 68|         self.model.to(self.device)\n 69|         self.model.eval()\n 70| \n 71|     @torch.inference_mode()\n 72|     def generate(\n 73|         self,\n 74|         prompt: str,\n 75|         *,\n 76|         max_new_tokens: int = 512,\n 77|         temperature: float = 0.2,\n 78|         top_p: float = 0.9,\n 79|     ) -> str:\n 80|         inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n 81|         outputs = self.model.generate(\n 82|             **inputs,\n 83|             do_sample=temperature > 0,\n 84|             temperature=temperature,\n 85|             top_p=top_p,\n 86|             max_new_tokens=max_new_tokens,\n 87|             pad_token_id=self.tokenizer.eos_token_id,\n 88|             eos_token_id=self.tokenizer.eos_token_id,\n 89|         )\n 90|         return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n 91| \n 92|     @torch.inference_mode()\n 93|     def embed(self, texts: str | Iterable[str]):\n 94|         if isinstance(texts, str):\n 95|             batch_texts = [texts]\n 96|         else:\n 97|             batch_texts = list(texts)\n 98|         if not batch_texts:\n 99|             raise ValueError(\"texts must not be empty\")\n100|         encoded = self.tokenizer(\n101|             batch_texts,\n102|             return_tensors=\"pt\",\n103|             padding=True,\n104|             truncation=True,\n105|         ).to(self.device)\n106|         model = self.model\n107|         outputs = model(\n108|             **encoded,\n109|             output_hidden_states=True,\n110|             use_cache=False,\n111|         )\n112|         last_hidden = outputs.hidden_states[-1]\n113|         mask = encoded[\"attention_mask\"].unsqueeze(-1)\n114|         summed = (last_hidden * mask).sum(dim=1)\n115|         counts = mask.sum(dim=1).clamp(min=1)\n116|         embeddings = summed / counts\n117|         return embeddings.cpu()\n118| '''\n119| \n120| README_MD = \"\"\"# LLM2Vec Wrapper (monGARS)\n121| \n122| This wrapper exposes:\n123| \n124| - `LLM2Vec.generate(prompt, ...)` → text generation\n125| - `LLM2Vec.embed(texts)` → embeddings via mean-pooled hidden states\n126| \n127| ## Quickstart\n128| \n129| ```python\n130| from llm2vec_wrapper import LLM2Vec\n131| wrapper = LLM2Vec(base_dir=\"..\", prefer_merged=False)\n132| print(wrapper.generate(\"Bonjour [MOD=Hippocampus] Rappelle-moi le dernier contexte.\"))\n133| vector = wrapper.embed(\"Allô, ça va?\")\n134| print(vector.shape)\n135| ```\n136| \"\"\"\n137| \n138| \n139| CONFIG_JSON = {\n140|     \"name\": \"monGARS-LLM2Vec\",\n141|     \"backbone\": \"Dolphin3.0-Llama3.1-8B\",\n142|     \"adapter\": \"lora_adapter\",\n143|     \"supports_merged\": True,\n144|     \"embed_strategy\": \"last_hidden_mean_pool\",\n145|     \"prompt_tag\": \"[MOD=<Module>]\",\n146| }\n147| \n148| \n149| def write_wrapper(model_dir: Path) -> None:\n150|     wrap_dir = model_dir / \"wrapper\"\n151|     wrap_dir.mkdir(parents=True, exist_ok=True)\n152| \n153|     (wrap_dir / \"llm2vec_wrapper.py\").write_text(WRAPPER_PY, encoding=\"utf-8\")\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"LLM2Vec.embed\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 39, "function": null, "signature": null, "prompt": "You are a senior engineer. Implement the missing logic near the specified line.\n\nConstraints:\n- Keep behaviour backward-compatible unless tests specify otherwise.\n- Use clear types, docstrings, and precise exceptions.\n- Add minimal unit tests if behaviour isn't covered.\n\nContext (numbered):\n14|     --val-file /path/to/val.jsonl \\\n15|     --out-dir out/monGARS_dolphin_multimodule \\\n16|     --epochs 2 --lr 1.5e-4 --cutoff-len 4096 \\\n17|     --merge-and-save  # optional full-weights export\n18| \n19| Requirements:\n20|   pip install \"unsloth>=2025.10.1\" \"transformers>=4.56.0\" \"datasets>=2.20.0\" \"accelerate>=0.34.0\" \"peft>=0.13.0\" torch\n21| \"\"\"\n22| from __future__ import annotations\n23| \n24| import argparse\n25| import json\n26| import logging\n27| import sys\n28| from pathlib import Path\n29| from typing import Any, Dict\n30| \n31| import torch\n32| from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n33| \n34| from datasets import load_dataset\n35| \n36| LOGGER = logging.getLogger(\"unsloth_multimodule\")\n37| \n38| BASE_MODEL = \"dphn/Dolphin3.0-Llama3.1-8B\"\n39| \n40| \n41| def _setup_logging():\n42|     logging.basicConfig(\n43|         level=logging.INFO,\n44|         format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n45|         handlers=[logging.StreamHandler(sys.stdout)],\n46|     )\n47| \n48| \n49| def _detect_device_map():\n50|     \"\"\"Return an automatic device map suitable for most environments.\"\"\"\n51| \n52|     return \"auto\"\n53| \n54| \n55| def _load_unsloth(base_model: str, max_len: int, try_4bit: bool = True):\n56|     from unsloth import FastLanguageModel\n57| \n58|     kwargs = dict(\n59|         model_name=base_model,\n60|         max_seq_length=max_len,\n61|         dtype=None,\n62|         device_map=_detect_device_map(),\n63|         trust_remote_code=True,\n64|     )\n\nDeliverables:\n- Updated code with a complete implementation.\n- Brief note explaining rationale and complexity assumptions.\n", "title": "Implement missing logic near L39 in scripts/train_dolphin_unsloth_multimodule.py"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 47, "function": "_setup_logging", "signature": "def _setup_logging():", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"_setup_logging\" in file \"scripts/train_dolphin_unsloth_multimodule.py\".\n\nSignature:\ndef _setup_logging():\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n  1| #!/usr/bin/env python3\n  2| \"\"\"\n  3| train_dolphin_unsloth_multimodule.py\n  4| \n  5| End-to-end fine-tuning for Dolphin 3.0 (Llama 3.1 8B) using Unsloth + LoRA.\n  6| - Ingests your uploaded train/val JSONL (supports prompt/response, instruction/input/output, or messages[])\n  7| - Works with module-tagged prompts like: [MOD=Cortex], [MOD=Hippocampus], etc.\n  8| - Trains with 4-bit base + LoRA, with safe fallbacks\n  9| - Exports a minimal LLM2Vec-style wrapper (generate + embed via mean pooling)\n 10| \n 11| USAGE (typical):\n 12|   python scripts/train_dolphin_unsloth_multimodule.py \\\n 13|     --train-file /path/to/train.jsonl \\\n 14|     --val-file /path/to/val.jsonl \\\n 15|     --out-dir out/monGARS_dolphin_multimodule \\\n 16|     --epochs 2 --lr 1.5e-4 --cutoff-len 4096 \\\n 17|     --merge-and-save  # optional full-weights export\n 18| \n 19| Requirements:\n 20|   pip install \"unsloth>=2025.10.1\" \"transformers>=4.56.0\" \"datasets>=2.20.0\" \"accelerate>=0.34.0\" \"peft>=0.13.0\" torch\n 21| \"\"\"\n 22| from __future__ import annotations\n 23| \n 24| import argparse\n 25| import json\n 26| import logging\n 27| import sys\n 28| from pathlib import Path\n 29| from typing import Any, Dict\n 30| \n 31| import torch\n 32| from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n 33| \n 34| from datasets import load_dataset\n 35| \n 36| LOGGER = logging.getLogger(\"unsloth_multimodule\")\n 37| \n 38| BASE_MODEL = \"dphn/Dolphin3.0-Llama3.1-8B\"\n 39| \n 40| \n 41| def _setup_logging():\n 42|     logging.basicConfig(\n 43|         level=logging.INFO,\n 44|         format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n 45|         handlers=[logging.StreamHandler(sys.stdout)],\n 46|     )\n 47| \n 48| \n 49| def _detect_device_map():\n 50|     \"\"\"Return an automatic device map suitable for most environments.\"\"\"\n 51| \n 52|     return \"auto\"\n 53| \n 54| \n 55| def _load_unsloth(base_model: str, max_len: int, try_4bit: bool = True):\n 56|     from unsloth import FastLanguageModel\n 57| \n 58|     kwargs = dict(\n 59|         model_name=base_model,\n 60|         max_seq_length=max_len,\n 61|         dtype=None,\n 62|         device_map=_detect_device_map(),\n 63|         trust_remote_code=True,\n 64|     )\n 65|     if try_4bit:\n 66|         kwargs[\"load_in_4bit\"] = True\n 67|     return FastLanguageModel.from_pretrained(**kwargs)\n 68| \n 69| \n 70| def _get_peft(model, r: int = 8, alpha: int = 16, dropout: float = 0.05):\n 71|     from unsloth import FastLanguageModel\n 72| \n 73|     return FastLanguageModel.get_peft_model(\n 74|         model,\n 75|         r=r,\n 76|         lora_alpha=alpha,\n 77|         lora_dropout=dropout,\n 78|         target_modules=\"all-linear\",\n 79|         bias=\"none\",\n 80|         use_gradient_checkpointing=True,\n 81|     )\n 82| \n 83| \n 84| # ---------- Data loading / normalization ----------\n 85| def _normalize_item(rec: Dict[str, Any]) -> Dict[str, str] | None:\n 86|     \"\"\"\n 87|     Accepts any of:\n 88|       {prompt, response}\n 89|       {instruction, input, output}\n 90|       {messages: [{role, content}, ...]}\n 91|     Returns dict with keys: instruction, input, output\n 92|     \"\"\"\n 93| \n 94|     if \"messages\" in rec and isinstance(rec[\"messages\"], list):\n 95|         msgs = rec[\"messages\"]\n 96|         instr = None\n 97|         inp = \"\"\n 98|         out = None\n 99|         user_parts = []\n100|         for m in msgs:\n101|             role = (m.get(\"role\") or \"\").lower()\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"_setup_logging\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 53, "function": "_detect_device_map", "signature": "def _detect_device_map():", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"_detect_device_map\" in file \"scripts/train_dolphin_unsloth_multimodule.py\".\n\nSignature:\ndef _detect_device_map():\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n  9| - Exports a minimal LLM2Vec-style wrapper (generate + embed via mean pooling)\n 10| \n 11| USAGE (typical):\n 12|   python scripts/train_dolphin_unsloth_multimodule.py \\\n 13|     --train-file /path/to/train.jsonl \\\n 14|     --val-file /path/to/val.jsonl \\\n 15|     --out-dir out/monGARS_dolphin_multimodule \\\n 16|     --epochs 2 --lr 1.5e-4 --cutoff-len 4096 \\\n 17|     --merge-and-save  # optional full-weights export\n 18| \n 19| Requirements:\n 20|   pip install \"unsloth>=2025.10.1\" \"transformers>=4.56.0\" \"datasets>=2.20.0\" \"accelerate>=0.34.0\" \"peft>=0.13.0\" torch\n 21| \"\"\"\n 22| from __future__ import annotations\n 23| \n 24| import argparse\n 25| import json\n 26| import logging\n 27| import sys\n 28| from pathlib import Path\n 29| from typing import Any, Dict\n 30| \n 31| import torch\n 32| from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n 33| \n 34| from datasets import load_dataset\n 35| \n 36| LOGGER = logging.getLogger(\"unsloth_multimodule\")\n 37| \n 38| BASE_MODEL = \"dphn/Dolphin3.0-Llama3.1-8B\"\n 39| \n 40| \n 41| def _setup_logging():\n 42|     logging.basicConfig(\n 43|         level=logging.INFO,\n 44|         format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n 45|         handlers=[logging.StreamHandler(sys.stdout)],\n 46|     )\n 47| \n 48| \n 49| def _detect_device_map():\n 50|     \"\"\"Return an automatic device map suitable for most environments.\"\"\"\n 51| \n 52|     return \"auto\"\n 53| \n 54| \n 55| def _load_unsloth(base_model: str, max_len: int, try_4bit: bool = True):\n 56|     from unsloth import FastLanguageModel\n 57| \n 58|     kwargs = dict(\n 59|         model_name=base_model,\n 60|         max_seq_length=max_len,\n 61|         dtype=None,\n 62|         device_map=_detect_device_map(),\n 63|         trust_remote_code=True,\n 64|     )\n 65|     if try_4bit:\n 66|         kwargs[\"load_in_4bit\"] = True\n 67|     return FastLanguageModel.from_pretrained(**kwargs)\n 68| \n 69| \n 70| def _get_peft(model, r: int = 8, alpha: int = 16, dropout: float = 0.05):\n 71|     from unsloth import FastLanguageModel\n 72| \n 73|     return FastLanguageModel.get_peft_model(\n 74|         model,\n 75|         r=r,\n 76|         lora_alpha=alpha,\n 77|         lora_dropout=dropout,\n 78|         target_modules=\"all-linear\",\n 79|         bias=\"none\",\n 80|         use_gradient_checkpointing=True,\n 81|     )\n 82| \n 83| \n 84| # ---------- Data loading / normalization ----------\n 85| def _normalize_item(rec: Dict[str, Any]) -> Dict[str, str] | None:\n 86|     \"\"\"\n 87|     Accepts any of:\n 88|       {prompt, response}\n 89|       {instruction, input, output}\n 90|       {messages: [{role, content}, ...]}\n 91|     Returns dict with keys: instruction, input, output\n 92|     \"\"\"\n 93| \n 94|     if \"messages\" in rec and isinstance(rec[\"messages\"], list):\n 95|         msgs = rec[\"messages\"]\n 96|         instr = None\n 97|         inp = \"\"\n 98|         out = None\n 99|         user_parts = []\n100|         for m in msgs:\n101|             role = (m.get(\"role\") or \"\").lower()\n102|             content = (m.get(\"content\") or \"\").strip()\n103|             if role == \"user\":\n104|                 user_parts.append(content)\n105|             elif role == \"assistant\":\n106|                 out = content\n107|         if user_parts:\n108|             instr = \"\\n\\n\".join(user_parts)\n109|         if instr and out:\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"_detect_device_map\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 68, "function": "_load_unsloth", "signature": "def _load_unsloth(base_model: str, max_len: int, try_4bit: bool = True):", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"_load_unsloth\" in file \"scripts/train_dolphin_unsloth_multimodule.py\".\n\nSignature:\ndef _load_unsloth(base_model: str, max_len: int, try_4bit: bool = True):\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n 15|     --out-dir out/monGARS_dolphin_multimodule \\\n 16|     --epochs 2 --lr 1.5e-4 --cutoff-len 4096 \\\n 17|     --merge-and-save  # optional full-weights export\n 18| \n 19| Requirements:\n 20|   pip install \"unsloth>=2025.10.1\" \"transformers>=4.56.0\" \"datasets>=2.20.0\" \"accelerate>=0.34.0\" \"peft>=0.13.0\" torch\n 21| \"\"\"\n 22| from __future__ import annotations\n 23| \n 24| import argparse\n 25| import json\n 26| import logging\n 27| import sys\n 28| from pathlib import Path\n 29| from typing import Any, Dict\n 30| \n 31| import torch\n 32| from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n 33| \n 34| from datasets import load_dataset\n 35| \n 36| LOGGER = logging.getLogger(\"unsloth_multimodule\")\n 37| \n 38| BASE_MODEL = \"dphn/Dolphin3.0-Llama3.1-8B\"\n 39| \n 40| \n 41| def _setup_logging():\n 42|     logging.basicConfig(\n 43|         level=logging.INFO,\n 44|         format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n 45|         handlers=[logging.StreamHandler(sys.stdout)],\n 46|     )\n 47| \n 48| \n 49| def _detect_device_map():\n 50|     \"\"\"Return an automatic device map suitable for most environments.\"\"\"\n 51| \n 52|     return \"auto\"\n 53| \n 54| \n 55| def _load_unsloth(base_model: str, max_len: int, try_4bit: bool = True):\n 56|     from unsloth import FastLanguageModel\n 57| \n 58|     kwargs = dict(\n 59|         model_name=base_model,\n 60|         max_seq_length=max_len,\n 61|         dtype=None,\n 62|         device_map=_detect_device_map(),\n 63|         trust_remote_code=True,\n 64|     )\n 65|     if try_4bit:\n 66|         kwargs[\"load_in_4bit\"] = True\n 67|     return FastLanguageModel.from_pretrained(**kwargs)\n 68| \n 69| \n 70| def _get_peft(model, r: int = 8, alpha: int = 16, dropout: float = 0.05):\n 71|     from unsloth import FastLanguageModel\n 72| \n 73|     return FastLanguageModel.get_peft_model(\n 74|         model,\n 75|         r=r,\n 76|         lora_alpha=alpha,\n 77|         lora_dropout=dropout,\n 78|         target_modules=\"all-linear\",\n 79|         bias=\"none\",\n 80|         use_gradient_checkpointing=True,\n 81|     )\n 82| \n 83| \n 84| # ---------- Data loading / normalization ----------\n 85| def _normalize_item(rec: Dict[str, Any]) -> Dict[str, str] | None:\n 86|     \"\"\"\n 87|     Accepts any of:\n 88|       {prompt, response}\n 89|       {instruction, input, output}\n 90|       {messages: [{role, content}, ...]}\n 91|     Returns dict with keys: instruction, input, output\n 92|     \"\"\"\n 93| \n 94|     if \"messages\" in rec and isinstance(rec[\"messages\"], list):\n 95|         msgs = rec[\"messages\"]\n 96|         instr = None\n 97|         inp = \"\"\n 98|         out = None\n 99|         user_parts = []\n100|         for m in msgs:\n101|             role = (m.get(\"role\") or \"\").lower()\n102|             content = (m.get(\"content\") or \"\").strip()\n103|             if role == \"user\":\n104|                 user_parts.append(content)\n105|             elif role == \"assistant\":\n106|                 out = content\n107|         if user_parts:\n108|             instr = \"\\n\\n\".join(user_parts)\n109|         if instr and out:\n110|             return {\"instruction\": instr, \"input\": \"\", \"output\": out}\n111| \n112|     if \"prompt\" in rec and \"response\" in rec:\n113|         instr = (rec.get(\"prompt\") or \"\").strip()\n114|         out = (rec.get(\"response\") or \"\").strip()\n115|         if instr and out:\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"_load_unsloth\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 85, "function": "_get_peft", "signature": "def _get_peft(model, r: int = 8, alpha: int = 16, dropout: float = 0.05):", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"_get_peft\" in file \"scripts/train_dolphin_unsloth_multimodule.py\".\n\nSignature:\ndef _get_peft(model, r: int = 8, alpha: int = 16, dropout: float = 0.05):\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n 30| \n 31| import torch\n 32| from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n 33| \n 34| from datasets import load_dataset\n 35| \n 36| LOGGER = logging.getLogger(\"unsloth_multimodule\")\n 37| \n 38| BASE_MODEL = \"dphn/Dolphin3.0-Llama3.1-8B\"\n 39| \n 40| \n 41| def _setup_logging():\n 42|     logging.basicConfig(\n 43|         level=logging.INFO,\n 44|         format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n 45|         handlers=[logging.StreamHandler(sys.stdout)],\n 46|     )\n 47| \n 48| \n 49| def _detect_device_map():\n 50|     \"\"\"Return an automatic device map suitable for most environments.\"\"\"\n 51| \n 52|     return \"auto\"\n 53| \n 54| \n 55| def _load_unsloth(base_model: str, max_len: int, try_4bit: bool = True):\n 56|     from unsloth import FastLanguageModel\n 57| \n 58|     kwargs = dict(\n 59|         model_name=base_model,\n 60|         max_seq_length=max_len,\n 61|         dtype=None,\n 62|         device_map=_detect_device_map(),\n 63|         trust_remote_code=True,\n 64|     )\n 65|     if try_4bit:\n 66|         kwargs[\"load_in_4bit\"] = True\n 67|     return FastLanguageModel.from_pretrained(**kwargs)\n 68| \n 69| \n 70| def _get_peft(model, r: int = 8, alpha: int = 16, dropout: float = 0.05):\n 71|     from unsloth import FastLanguageModel\n 72| \n 73|     return FastLanguageModel.get_peft_model(\n 74|         model,\n 75|         r=r,\n 76|         lora_alpha=alpha,\n 77|         lora_dropout=dropout,\n 78|         target_modules=\"all-linear\",\n 79|         bias=\"none\",\n 80|         use_gradient_checkpointing=True,\n 81|     )\n 82| \n 83| \n 84| # ---------- Data loading / normalization ----------\n 85| def _normalize_item(rec: Dict[str, Any]) -> Dict[str, str] | None:\n 86|     \"\"\"\n 87|     Accepts any of:\n 88|       {prompt, response}\n 89|       {instruction, input, output}\n 90|       {messages: [{role, content}, ...]}\n 91|     Returns dict with keys: instruction, input, output\n 92|     \"\"\"\n 93| \n 94|     if \"messages\" in rec and isinstance(rec[\"messages\"], list):\n 95|         msgs = rec[\"messages\"]\n 96|         instr = None\n 97|         inp = \"\"\n 98|         out = None\n 99|         user_parts = []\n100|         for m in msgs:\n101|             role = (m.get(\"role\") or \"\").lower()\n102|             content = (m.get(\"content\") or \"\").strip()\n103|             if role == \"user\":\n104|                 user_parts.append(content)\n105|             elif role == \"assistant\":\n106|                 out = content\n107|         if user_parts:\n108|             instr = \"\\n\\n\".join(user_parts)\n109|         if instr and out:\n110|             return {\"instruction\": instr, \"input\": \"\", \"output\": out}\n111| \n112|     if \"prompt\" in rec and \"response\" in rec:\n113|         instr = (rec.get(\"prompt\") or \"\").strip()\n114|         out = (rec.get(\"response\") or \"\").strip()\n115|         if instr and out:\n116|             return {\"instruction\": instr, \"input\": \"\", \"output\": out}\n117| \n118|     if \"instruction\" in rec and \"output\" in rec:\n119|         instr = (rec.get(\"instruction\") or \"\").strip()\n120|         inp = (\n121|             (rec.get(\"input\") or \"\").strip()\n122|             if isinstance(rec.get(\"input\"), str)\n123|             else \"\"\n124|         )\n125|         out = rec.get(\"output\")\n126|         if not isinstance(out, str):\n127|             out = json.dumps(out, ensure_ascii=False, separators=(\",\", \":\"))\n128|         out = out.strip()\n129|         if instr and out:\n130|             return {\"instruction\": instr, \"input\": inp, \"output\": out}\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"_get_peft\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 133, "function": "_get_peft", "signature": "def _get_peft(model, r: int = 8, alpha: int = 16, dropout: float = 0.05):", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"_get_peft\" in file \"scripts/train_dolphin_unsloth_multimodule.py\".\n\nSignature:\ndef _get_peft(model, r: int = 8, alpha: int = 16, dropout: float = 0.05):\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n 30| \n 31| import torch\n 32| from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n 33| \n 34| from datasets import load_dataset\n 35| \n 36| LOGGER = logging.getLogger(\"unsloth_multimodule\")\n 37| \n 38| BASE_MODEL = \"dphn/Dolphin3.0-Llama3.1-8B\"\n 39| \n 40| \n 41| def _setup_logging():\n 42|     logging.basicConfig(\n 43|         level=logging.INFO,\n 44|         format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n 45|         handlers=[logging.StreamHandler(sys.stdout)],\n 46|     )\n 47| \n 48| \n 49| def _detect_device_map():\n 50|     \"\"\"Return an automatic device map suitable for most environments.\"\"\"\n 51| \n 52|     return \"auto\"\n 53| \n 54| \n 55| def _load_unsloth(base_model: str, max_len: int, try_4bit: bool = True):\n 56|     from unsloth import FastLanguageModel\n 57| \n 58|     kwargs = dict(\n 59|         model_name=base_model,\n 60|         max_seq_length=max_len,\n 61|         dtype=None,\n 62|         device_map=_detect_device_map(),\n 63|         trust_remote_code=True,\n 64|     )\n 65|     if try_4bit:\n 66|         kwargs[\"load_in_4bit\"] = True\n 67|     return FastLanguageModel.from_pretrained(**kwargs)\n 68| \n 69| \n 70| def _get_peft(model, r: int = 8, alpha: int = 16, dropout: float = 0.05):\n 71|     from unsloth import FastLanguageModel\n 72| \n 73|     return FastLanguageModel.get_peft_model(\n 74|         model,\n 75|         r=r,\n 76|         lora_alpha=alpha,\n 77|         lora_dropout=dropout,\n 78|         target_modules=\"all-linear\",\n 79|         bias=\"none\",\n 80|         use_gradient_checkpointing=True,\n 81|     )\n 82| \n 83| \n 84| # ---------- Data loading / normalization ----------\n 85| def _normalize_item(rec: Dict[str, Any]) -> Dict[str, str] | None:\n 86|     \"\"\"\n 87|     Accepts any of:\n 88|       {prompt, response}\n 89|       {instruction, input, output}\n 90|       {messages: [{role, content}, ...]}\n 91|     Returns dict with keys: instruction, input, output\n 92|     \"\"\"\n 93| \n 94|     if \"messages\" in rec and isinstance(rec[\"messages\"], list):\n 95|         msgs = rec[\"messages\"]\n 96|         instr = None\n 97|         inp = \"\"\n 98|         out = None\n 99|         user_parts = []\n100|         for m in msgs:\n101|             role = (m.get(\"role\") or \"\").lower()\n102|             content = (m.get(\"content\") or \"\").strip()\n103|             if role == \"user\":\n104|                 user_parts.append(content)\n105|             elif role == \"assistant\":\n106|                 out = content\n107|         if user_parts:\n108|             instr = \"\\n\\n\".join(user_parts)\n109|         if instr and out:\n110|             return {\"instruction\": instr, \"input\": \"\", \"output\": out}\n111| \n112|     if \"prompt\" in rec and \"response\" in rec:\n113|         instr = (rec.get(\"prompt\") or \"\").strip()\n114|         out = (rec.get(\"response\") or \"\").strip()\n115|         if instr and out:\n116|             return {\"instruction\": instr, \"input\": \"\", \"output\": out}\n117| \n118|     if \"instruction\" in rec and \"output\" in rec:\n119|         instr = (rec.get(\"instruction\") or \"\").strip()\n120|         inp = (\n121|             (rec.get(\"input\") or \"\").strip()\n122|             if isinstance(rec.get(\"input\"), str)\n123|             else \"\"\n124|         )\n125|         out = rec.get(\"output\")\n126|         if not isinstance(out, str):\n127|             out = json.dumps(out, ensure_ascii=False, separators=(\",\", \":\"))\n128|         out = out.strip()\n129|         if instr and out:\n130|             return {\"instruction\": instr, \"input\": inp, \"output\": out}\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"_get_peft\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 181, "function": "_tokenize", "signature": "def _tokenize(tokenizer, ds, max_len: int):", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"_tokenize\" in file \"scripts/train_dolphin_unsloth_multimodule.py\".\n\nSignature:\ndef _tokenize(tokenizer, ds, max_len: int):\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n141|     sysmsg = (\n142|         \"You are monGARS internal assistant. \"\n143|         \"Follow the module contract indicated by tags like [MOD=Cortex], [MOD=Hippocampus], etc. \"\n144|         \"Do not speculate beyond module specifications.\"\n145|     )\n146|     prompt = (\n147|         f\"<|im_start|>system\\n{sysmsg}<|im_end|>\\n\"\n148|         f\"<|im_start|>user\\n{user}<|im_end|>\\n\"\n149|         f\"<|im_start|>assistant\\n\"\n150|     )\n151|     return {\"text\": prompt + example[\"output\"] + \"<|im_end|>\\n\"}\n152| \n153| \n154| def _load_as_dataset(train_file: str, val_file: str | None):\n155|     \"\"\"Load JSONL files into HF datasets and normalize records.\"\"\"\n156| \n157|     data_files = {\"train\": train_file}\n158|     if val_file:\n159|         data_files[\"validation\"] = val_file\n160|     raw = load_dataset(\"json\", data_files=data_files)\n161| \n162|     for split in list(raw.keys()):\n163|         raw[split] = (\n164|             raw[split]\n165|             .map(\n166|                 lambda x: _normalize_item(x),\n167|                 remove_columns=raw[split].column_names,\n168|             )\n169|             .filter(lambda r: r is not None)\n170|         )\n171| \n172|     for split in list(raw.keys()):\n173|         raw[split] = raw[split].map(\n174|             _format_prompt_for_chat,\n175|             remove_columns=raw[split].column_names,\n176|         )\n177|     return raw\n178| \n179| \n180| # ---------- Tokenization ----------\n181| def _tokenize(tokenizer, ds, max_len: int):\n182|     def _tok(batch):\n183|         return tokenizer(batch[\"text\"], truncation=True, max_length=max_len)\n184| \n185|     out = {}\n186|     for split in list(ds.keys()):\n187|         out[split] = ds[split].map(_tok, batched=True, remove_columns=[\"text\"])\n188|     return out\n189| \n190| \n191| # ---------- LLM2Vec-style wrapper export ----------\n192| LLM2VEC_PY = r'''# llm2vec_wrapper.py\n193| import torch\n194| from pathlib import Path\n195| from transformers import AutoModelForCausalLM, AutoTokenizer\n196| try:\n197|     from peft import PeftModel\n198| except Exception:\n199|     PeftModel = None\n200| \n201| \n202| class LLM2Vec:\n203|     \"\"\"\n204|     Minimal chat + embed wrapper.\n205|     - generate(prompt, ...) -> str\n206|     - embed(texts) -> torch.Tensor [N, hidden_size] (mean-pooled last layer)\n207|     \"\"\"\n208| \n209|     def __init__(self, base_dir, prefer_merged=False, device=None, load_in_4bit=True):\n210|         self.base_dir = str(base_dir)\n211|         self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n212|         tok_dir = f\"{self.base_dir}/tokenizer\"\n213|         self.tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True)\n214| \n215|         if prefer_merged and (Path(f\"{self.base_dir}/merged\").exists()):\n216|             model_dir = f\"{self.base_dir}/merged\"\n217|             self.model = AutoModelForCausalLM.from_pretrained(\n218|                 model_dir,\n219|                 torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n220|                 device_map=\"auto\",\n221|             )\n222|         else:\n223|             base_model = \"dphn/Dolphin3.0-Llama3.1-8B\"\n224|             self.model = AutoModelForCausalLM.from_pretrained(\n225|                 base_model,\n226|                 load_in_4bit=load_in_4bit,\n227|                 device_map=\"auto\",\n228|                 trust_remote_code=True,\n229|             )\n230|             if PeftModel is None:\n231|                 raise RuntimeError(\"peft not available; cannot load LoRA adapter.\")\n232|             self.model = PeftModel.from_pretrained(self.model, f\"{self.base_dir}/lora_adapter\")\n233|         self.model.eval()\n234| \n235|     @torch.inference_mode()\n236|     def generate(self, prompt, max_new_tokens=512, temperature=0.2, top_p=0.9):\n237|         inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n238|         out = self.model.generate(\n239|             **inputs,\n240|             do_sample=temperature > 0,\n241|             temperature=temperature,\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"_tokenize\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 182, "function": "_tok", "signature": "def _tok(batch):", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"_tok\" in file \"scripts/train_dolphin_unsloth_multimodule.py\".\n\nSignature:\ndef _tok(batch):\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n142|         \"You are monGARS internal assistant. \"\n143|         \"Follow the module contract indicated by tags like [MOD=Cortex], [MOD=Hippocampus], etc. \"\n144|         \"Do not speculate beyond module specifications.\"\n145|     )\n146|     prompt = (\n147|         f\"<|im_start|>system\\n{sysmsg}<|im_end|>\\n\"\n148|         f\"<|im_start|>user\\n{user}<|im_end|>\\n\"\n149|         f\"<|im_start|>assistant\\n\"\n150|     )\n151|     return {\"text\": prompt + example[\"output\"] + \"<|im_end|>\\n\"}\n152| \n153| \n154| def _load_as_dataset(train_file: str, val_file: str | None):\n155|     \"\"\"Load JSONL files into HF datasets and normalize records.\"\"\"\n156| \n157|     data_files = {\"train\": train_file}\n158|     if val_file:\n159|         data_files[\"validation\"] = val_file\n160|     raw = load_dataset(\"json\", data_files=data_files)\n161| \n162|     for split in list(raw.keys()):\n163|         raw[split] = (\n164|             raw[split]\n165|             .map(\n166|                 lambda x: _normalize_item(x),\n167|                 remove_columns=raw[split].column_names,\n168|             )\n169|             .filter(lambda r: r is not None)\n170|         )\n171| \n172|     for split in list(raw.keys()):\n173|         raw[split] = raw[split].map(\n174|             _format_prompt_for_chat,\n175|             remove_columns=raw[split].column_names,\n176|         )\n177|     return raw\n178| \n179| \n180| # ---------- Tokenization ----------\n181| def _tokenize(tokenizer, ds, max_len: int):\n182|     def _tok(batch):\n183|         return tokenizer(batch[\"text\"], truncation=True, max_length=max_len)\n184| \n185|     out = {}\n186|     for split in list(ds.keys()):\n187|         out[split] = ds[split].map(_tok, batched=True, remove_columns=[\"text\"])\n188|     return out\n189| \n190| \n191| # ---------- LLM2Vec-style wrapper export ----------\n192| LLM2VEC_PY = r'''# llm2vec_wrapper.py\n193| import torch\n194| from pathlib import Path\n195| from transformers import AutoModelForCausalLM, AutoTokenizer\n196| try:\n197|     from peft import PeftModel\n198| except Exception:\n199|     PeftModel = None\n200| \n201| \n202| class LLM2Vec:\n203|     \"\"\"\n204|     Minimal chat + embed wrapper.\n205|     - generate(prompt, ...) -> str\n206|     - embed(texts) -> torch.Tensor [N, hidden_size] (mean-pooled last layer)\n207|     \"\"\"\n208| \n209|     def __init__(self, base_dir, prefer_merged=False, device=None, load_in_4bit=True):\n210|         self.base_dir = str(base_dir)\n211|         self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n212|         tok_dir = f\"{self.base_dir}/tokenizer\"\n213|         self.tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True)\n214| \n215|         if prefer_merged and (Path(f\"{self.base_dir}/merged\").exists()):\n216|             model_dir = f\"{self.base_dir}/merged\"\n217|             self.model = AutoModelForCausalLM.from_pretrained(\n218|                 model_dir,\n219|                 torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n220|                 device_map=\"auto\",\n221|             )\n222|         else:\n223|             base_model = \"dphn/Dolphin3.0-Llama3.1-8B\"\n224|             self.model = AutoModelForCausalLM.from_pretrained(\n225|                 base_model,\n226|                 load_in_4bit=load_in_4bit,\n227|                 device_map=\"auto\",\n228|                 trust_remote_code=True,\n229|             )\n230|             if PeftModel is None:\n231|                 raise RuntimeError(\"peft not available; cannot load LoRA adapter.\")\n232|             self.model = PeftModel.from_pretrained(self.model, f\"{self.base_dir}/lora_adapter\")\n233|         self.model.eval()\n234| \n235|     @torch.inference_mode()\n236|     def generate(self, prompt, max_new_tokens=512, temperature=0.2, top_p=0.9):\n237|         inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n238|         out = self.model.generate(\n239|             **inputs,\n240|             do_sample=temperature > 0,\n241|             temperature=temperature,\n242|             top_p=top_p,\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"_tok\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 208, "function": "_tok", "signature": "def _tok(batch):", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"_tok\" in file \"scripts/train_dolphin_unsloth_multimodule.py\".\n\nSignature:\ndef _tok(batch):\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n142|         \"You are monGARS internal assistant. \"\n143|         \"Follow the module contract indicated by tags like [MOD=Cortex], [MOD=Hippocampus], etc. \"\n144|         \"Do not speculate beyond module specifications.\"\n145|     )\n146|     prompt = (\n147|         f\"<|im_start|>system\\n{sysmsg}<|im_end|>\\n\"\n148|         f\"<|im_start|>user\\n{user}<|im_end|>\\n\"\n149|         f\"<|im_start|>assistant\\n\"\n150|     )\n151|     return {\"text\": prompt + example[\"output\"] + \"<|im_end|>\\n\"}\n152| \n153| \n154| def _load_as_dataset(train_file: str, val_file: str | None):\n155|     \"\"\"Load JSONL files into HF datasets and normalize records.\"\"\"\n156| \n157|     data_files = {\"train\": train_file}\n158|     if val_file:\n159|         data_files[\"validation\"] = val_file\n160|     raw = load_dataset(\"json\", data_files=data_files)\n161| \n162|     for split in list(raw.keys()):\n163|         raw[split] = (\n164|             raw[split]\n165|             .map(\n166|                 lambda x: _normalize_item(x),\n167|                 remove_columns=raw[split].column_names,\n168|             )\n169|             .filter(lambda r: r is not None)\n170|         )\n171| \n172|     for split in list(raw.keys()):\n173|         raw[split] = raw[split].map(\n174|             _format_prompt_for_chat,\n175|             remove_columns=raw[split].column_names,\n176|         )\n177|     return raw\n178| \n179| \n180| # ---------- Tokenization ----------\n181| def _tokenize(tokenizer, ds, max_len: int):\n182|     def _tok(batch):\n183|         return tokenizer(batch[\"text\"], truncation=True, max_length=max_len)\n184| \n185|     out = {}\n186|     for split in list(ds.keys()):\n187|         out[split] = ds[split].map(_tok, batched=True, remove_columns=[\"text\"])\n188|     return out\n189| \n190| \n191| # ---------- LLM2Vec-style wrapper export ----------\n192| LLM2VEC_PY = r'''# llm2vec_wrapper.py\n193| import torch\n194| from pathlib import Path\n195| from transformers import AutoModelForCausalLM, AutoTokenizer\n196| try:\n197|     from peft import PeftModel\n198| except Exception:\n199|     PeftModel = None\n200| \n201| \n202| class LLM2Vec:\n203|     \"\"\"\n204|     Minimal chat + embed wrapper.\n205|     - generate(prompt, ...) -> str\n206|     - embed(texts) -> torch.Tensor [N, hidden_size] (mean-pooled last layer)\n207|     \"\"\"\n208| \n209|     def __init__(self, base_dir, prefer_merged=False, device=None, load_in_4bit=True):\n210|         self.base_dir = str(base_dir)\n211|         self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n212|         tok_dir = f\"{self.base_dir}/tokenizer\"\n213|         self.tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True)\n214| \n215|         if prefer_merged and (Path(f\"{self.base_dir}/merged\").exists()):\n216|             model_dir = f\"{self.base_dir}/merged\"\n217|             self.model = AutoModelForCausalLM.from_pretrained(\n218|                 model_dir,\n219|                 torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n220|                 device_map=\"auto\",\n221|             )\n222|         else:\n223|             base_model = \"dphn/Dolphin3.0-Llama3.1-8B\"\n224|             self.model = AutoModelForCausalLM.from_pretrained(\n225|                 base_model,\n226|                 load_in_4bit=load_in_4bit,\n227|                 device_map=\"auto\",\n228|                 trust_remote_code=True,\n229|             )\n230|             if PeftModel is None:\n231|                 raise RuntimeError(\"peft not available; cannot load LoRA adapter.\")\n232|             self.model = PeftModel.from_pretrained(self.model, f\"{self.base_dir}/lora_adapter\")\n233|         self.model.eval()\n234| \n235|     @torch.inference_mode()\n236|     def generate(self, prompt, max_new_tokens=512, temperature=0.2, top_p=0.9):\n237|         inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n238|         out = self.model.generate(\n239|             **inputs,\n240|             do_sample=temperature > 0,\n241|             temperature=temperature,\n242|             top_p=top_p,\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"_tok\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 236, "function": "LLM2Vec.generate", "signature": "def generate(self, prompt, max_new_tokens=512, temperature=0.2, top_p=0.9):", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"LLM2Vec.generate\" in file \"scripts/train_dolphin_unsloth_multimodule.py\".\n\nSignature:\ndef generate(self, prompt, max_new_tokens=512, temperature=0.2, top_p=0.9):\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n196| try:\n197|     from peft import PeftModel\n198| except Exception:\n199|     PeftModel = None\n200| \n201| \n202| class LLM2Vec:\n203|     \"\"\"\n204|     Minimal chat + embed wrapper.\n205|     - generate(prompt, ...) -> str\n206|     - embed(texts) -> torch.Tensor [N, hidden_size] (mean-pooled last layer)\n207|     \"\"\"\n208| \n209|     def __init__(self, base_dir, prefer_merged=False, device=None, load_in_4bit=True):\n210|         self.base_dir = str(base_dir)\n211|         self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n212|         tok_dir = f\"{self.base_dir}/tokenizer\"\n213|         self.tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True)\n214| \n215|         if prefer_merged and (Path(f\"{self.base_dir}/merged\").exists()):\n216|             model_dir = f\"{self.base_dir}/merged\"\n217|             self.model = AutoModelForCausalLM.from_pretrained(\n218|                 model_dir,\n219|                 torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n220|                 device_map=\"auto\",\n221|             )\n222|         else:\n223|             base_model = \"dphn/Dolphin3.0-Llama3.1-8B\"\n224|             self.model = AutoModelForCausalLM.from_pretrained(\n225|                 base_model,\n226|                 load_in_4bit=load_in_4bit,\n227|                 device_map=\"auto\",\n228|                 trust_remote_code=True,\n229|             )\n230|             if PeftModel is None:\n231|                 raise RuntimeError(\"peft not available; cannot load LoRA adapter.\")\n232|             self.model = PeftModel.from_pretrained(self.model, f\"{self.base_dir}/lora_adapter\")\n233|         self.model.eval()\n234| \n235|     @torch.inference_mode()\n236|     def generate(self, prompt, max_new_tokens=512, temperature=0.2, top_p=0.9):\n237|         inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n238|         out = self.model.generate(\n239|             **inputs,\n240|             do_sample=temperature > 0,\n241|             temperature=temperature,\n242|             top_p=top_p,\n243|             max_new_tokens=max_new_tokens,\n244|             pad_token_id=self.tokenizer.eos_token_id,\n245|             eos_token_id=self.tokenizer.eos_token_id,\n246|         )\n247|         return self.tokenizer.decode(out[0], skip_special_tokens=True)\n248| \n249|     @torch.inference_mode()\n250|     def embed(self, texts):\n251|         if isinstance(texts, str):\n252|             texts = [texts]\n253|         batch = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n254|         if hasattr(self.model, \"transformer\"):\n255|             outputs = self.model.transformer(**batch, output_hidden_states=True)\n256|         else:\n257|             outputs = self.model(**batch, output_hidden_states=True)\n258|         last = outputs.hidden_states[-1]\n259|         mask = batch[\"attention_mask\"].unsqueeze(-1)\n260|         summed = (last * mask).sum(dim=1)\n261|         counts = mask.sum(dim=1).clamp(min=1)\n262|         emb = summed / counts\n263|         return emb\n264| '''\n265| \n266| \n267| def _export_wrapper(out_dir: Path):\n268|     wrap = out_dir / \"wrapper\"\n269|     wrap.mkdir(parents=True, exist_ok=True)\n270|     (out_dir / \"tokenizer\").mkdir(exist_ok=True)\n271|     (wrap / \"llm2vec_wrapper.py\").write_text(LLM2VEC_PY, encoding=\"utf-8\")\n272|     (wrap / \"config.json\").write_text(\n273|         json.dumps(\n274|             {\n275|                 \"name\": \"monGARS-LLM2Vec\",\n276|                 \"backbone\": \"Dolphin3.0-Llama3.1-8B\",\n277|                 \"adapter_dir\": \"lora_adapter\",\n278|                 \"supports_merged\": True,\n279|                 \"embed_strategy\": \"last_hidden_mean_pool\",\n280|                 \"module_tag_format\": \"[MOD=<Module>]\",\n281|             },\n282|             indent=2,\n283|         ),\n284|         encoding=\"utf-8\",\n285|     )\n286|     (wrap / \"README.md\").write_text(\n287|         \"Minimal chat+embed wrapper. Usage:\\n\"\n288|         \"from llm2vec_wrapper import LLM2Vec\\n\"\n289|         \"w = LLM2Vec(base_dir='..', prefer_merged=False)\\n\"\n290|         \"print(w.generate('Bonjour [MOD=Hippocampus] Rappelle-moi le dernier contexte.'))\\n\"\n291|         \"vec = w.embed('On va au dépanneur.')\\n\"\n292|         \"print(vec.shape)\\n\",\n293|         encoding=\"utf-8\",\n294|     )\n295|     LOGGER.info(\"Wrapper bundle created at %s\", wrap)\n296|     return wrap\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"LLM2Vec.generate\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 250, "function": "LLM2Vec.embed", "signature": "def embed(self, texts):", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"LLM2Vec.embed\" in file \"scripts/train_dolphin_unsloth_multimodule.py\".\n\nSignature:\ndef embed(self, texts):\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n210|         self.base_dir = str(base_dir)\n211|         self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n212|         tok_dir = f\"{self.base_dir}/tokenizer\"\n213|         self.tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True)\n214| \n215|         if prefer_merged and (Path(f\"{self.base_dir}/merged\").exists()):\n216|             model_dir = f\"{self.base_dir}/merged\"\n217|             self.model = AutoModelForCausalLM.from_pretrained(\n218|                 model_dir,\n219|                 torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n220|                 device_map=\"auto\",\n221|             )\n222|         else:\n223|             base_model = \"dphn/Dolphin3.0-Llama3.1-8B\"\n224|             self.model = AutoModelForCausalLM.from_pretrained(\n225|                 base_model,\n226|                 load_in_4bit=load_in_4bit,\n227|                 device_map=\"auto\",\n228|                 trust_remote_code=True,\n229|             )\n230|             if PeftModel is None:\n231|                 raise RuntimeError(\"peft not available; cannot load LoRA adapter.\")\n232|             self.model = PeftModel.from_pretrained(self.model, f\"{self.base_dir}/lora_adapter\")\n233|         self.model.eval()\n234| \n235|     @torch.inference_mode()\n236|     def generate(self, prompt, max_new_tokens=512, temperature=0.2, top_p=0.9):\n237|         inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n238|         out = self.model.generate(\n239|             **inputs,\n240|             do_sample=temperature > 0,\n241|             temperature=temperature,\n242|             top_p=top_p,\n243|             max_new_tokens=max_new_tokens,\n244|             pad_token_id=self.tokenizer.eos_token_id,\n245|             eos_token_id=self.tokenizer.eos_token_id,\n246|         )\n247|         return self.tokenizer.decode(out[0], skip_special_tokens=True)\n248| \n249|     @torch.inference_mode()\n250|     def embed(self, texts):\n251|         if isinstance(texts, str):\n252|             texts = [texts]\n253|         batch = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n254|         if hasattr(self.model, \"transformer\"):\n255|             outputs = self.model.transformer(**batch, output_hidden_states=True)\n256|         else:\n257|             outputs = self.model(**batch, output_hidden_states=True)\n258|         last = outputs.hidden_states[-1]\n259|         mask = batch[\"attention_mask\"].unsqueeze(-1)\n260|         summed = (last * mask).sum(dim=1)\n261|         counts = mask.sum(dim=1).clamp(min=1)\n262|         emb = summed / counts\n263|         return emb\n264| '''\n265| \n266| \n267| def _export_wrapper(out_dir: Path):\n268|     wrap = out_dir / \"wrapper\"\n269|     wrap.mkdir(parents=True, exist_ok=True)\n270|     (out_dir / \"tokenizer\").mkdir(exist_ok=True)\n271|     (wrap / \"llm2vec_wrapper.py\").write_text(LLM2VEC_PY, encoding=\"utf-8\")\n272|     (wrap / \"config.json\").write_text(\n273|         json.dumps(\n274|             {\n275|                 \"name\": \"monGARS-LLM2Vec\",\n276|                 \"backbone\": \"Dolphin3.0-Llama3.1-8B\",\n277|                 \"adapter_dir\": \"lora_adapter\",\n278|                 \"supports_merged\": True,\n279|                 \"embed_strategy\": \"last_hidden_mean_pool\",\n280|                 \"module_tag_format\": \"[MOD=<Module>]\",\n281|             },\n282|             indent=2,\n283|         ),\n284|         encoding=\"utf-8\",\n285|     )\n286|     (wrap / \"README.md\").write_text(\n287|         \"Minimal chat+embed wrapper. Usage:\\n\"\n288|         \"from llm2vec_wrapper import LLM2Vec\\n\"\n289|         \"w = LLM2Vec(base_dir='..', prefer_merged=False)\\n\"\n290|         \"print(w.generate('Bonjour [MOD=Hippocampus] Rappelle-moi le dernier contexte.'))\\n\"\n291|         \"vec = w.embed('On va au dépanneur.')\\n\"\n292|         \"print(vec.shape)\\n\",\n293|         encoding=\"utf-8\",\n294|     )\n295|     LOGGER.info(\"Wrapper bundle created at %s\", wrap)\n296|     return wrap\n297| \n298| \n299| # ---------- Main train routine ----------\n300| def main():\n301|     _setup_logging()\n302|     ap = argparse.ArgumentParser()\n303|     ap.add_argument(\"--base-model\", default=BASE_MODEL)\n304|     ap.add_argument(\"--train-file\", required=True)\n305|     ap.add_argument(\"--val-file\", default=None)\n306|     ap.add_argument(\"--out-dir\", default=\"out/monGARS_dolphin_multimodule\")\n307|     ap.add_argument(\"--epochs\", type=int, default=2)\n308|     ap.add_argument(\"--lr\", type=float, default=1.5e-4)\n309|     ap.add_argument(\"--per-device-bs\", type=int, default=1)\n310|     ap.add_argument(\"--grad-accum\", type=int, default=8)\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"LLM2Vec.embed\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_dolphin_unsloth_multimodule.py", "line": 265, "function": "LLM2Vec.embed", "signature": "def embed(self, texts):", "prompt": "You are an expert Python engineer contributing to a production codebase.\n\nGoal:\nImplement the function \"LLM2Vec.embed\" in file \"scripts/train_dolphin_unsloth_multimodule.py\".\n\nSignature:\ndef embed(self, texts):\n\nNotes:\nNo explicit docstring available; infer behaviour from context, naming, and surrounding code.\n\nConstraints:\n- Maintain backwards-compatible public surface.\n- Use clear typing annotations and meaningful exceptions.\n- Keep the implementation efficient (time/space) and readable.\n- Add/adjust minimal tests for uncovered edge cases if necessary.\n\nContext (numbered lines):\n210|         self.base_dir = str(base_dir)\n211|         self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n212|         tok_dir = f\"{self.base_dir}/tokenizer\"\n213|         self.tokenizer = AutoTokenizer.from_pretrained(tok_dir, use_fast=True)\n214| \n215|         if prefer_merged and (Path(f\"{self.base_dir}/merged\").exists()):\n216|             model_dir = f\"{self.base_dir}/merged\"\n217|             self.model = AutoModelForCausalLM.from_pretrained(\n218|                 model_dir,\n219|                 torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n220|                 device_map=\"auto\",\n221|             )\n222|         else:\n223|             base_model = \"dphn/Dolphin3.0-Llama3.1-8B\"\n224|             self.model = AutoModelForCausalLM.from_pretrained(\n225|                 base_model,\n226|                 load_in_4bit=load_in_4bit,\n227|                 device_map=\"auto\",\n228|                 trust_remote_code=True,\n229|             )\n230|             if PeftModel is None:\n231|                 raise RuntimeError(\"peft not available; cannot load LoRA adapter.\")\n232|             self.model = PeftModel.from_pretrained(self.model, f\"{self.base_dir}/lora_adapter\")\n233|         self.model.eval()\n234| \n235|     @torch.inference_mode()\n236|     def generate(self, prompt, max_new_tokens=512, temperature=0.2, top_p=0.9):\n237|         inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n238|         out = self.model.generate(\n239|             **inputs,\n240|             do_sample=temperature > 0,\n241|             temperature=temperature,\n242|             top_p=top_p,\n243|             max_new_tokens=max_new_tokens,\n244|             pad_token_id=self.tokenizer.eos_token_id,\n245|             eos_token_id=self.tokenizer.eos_token_id,\n246|         )\n247|         return self.tokenizer.decode(out[0], skip_special_tokens=True)\n248| \n249|     @torch.inference_mode()\n250|     def embed(self, texts):\n251|         if isinstance(texts, str):\n252|             texts = [texts]\n253|         batch = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n254|         if hasattr(self.model, \"transformer\"):\n255|             outputs = self.model.transformer(**batch, output_hidden_states=True)\n256|         else:\n257|             outputs = self.model(**batch, output_hidden_states=True)\n258|         last = outputs.hidden_states[-1]\n259|         mask = batch[\"attention_mask\"].unsqueeze(-1)\n260|         summed = (last * mask).sum(dim=1)\n261|         counts = mask.sum(dim=1).clamp(min=1)\n262|         emb = summed / counts\n263|         return emb\n264| '''\n265| \n266| \n267| def _export_wrapper(out_dir: Path):\n268|     wrap = out_dir / \"wrapper\"\n269|     wrap.mkdir(parents=True, exist_ok=True)\n270|     (out_dir / \"tokenizer\").mkdir(exist_ok=True)\n271|     (wrap / \"llm2vec_wrapper.py\").write_text(LLM2VEC_PY, encoding=\"utf-8\")\n272|     (wrap / \"config.json\").write_text(\n273|         json.dumps(\n274|             {\n275|                 \"name\": \"monGARS-LLM2Vec\",\n276|                 \"backbone\": \"Dolphin3.0-Llama3.1-8B\",\n277|                 \"adapter_dir\": \"lora_adapter\",\n278|                 \"supports_merged\": True,\n279|                 \"embed_strategy\": \"last_hidden_mean_pool\",\n280|                 \"module_tag_format\": \"[MOD=<Module>]\",\n281|             },\n282|             indent=2,\n283|         ),\n284|         encoding=\"utf-8\",\n285|     )\n286|     (wrap / \"README.md\").write_text(\n287|         \"Minimal chat+embed wrapper. Usage:\\n\"\n288|         \"from llm2vec_wrapper import LLM2Vec\\n\"\n289|         \"w = LLM2Vec(base_dir='..', prefer_merged=False)\\n\"\n290|         \"print(w.generate('Bonjour [MOD=Hippocampus] Rappelle-moi le dernier contexte.'))\\n\"\n291|         \"vec = w.embed('On va au dépanneur.')\\n\"\n292|         \"print(vec.shape)\\n\",\n293|         encoding=\"utf-8\",\n294|     )\n295|     LOGGER.info(\"Wrapper bundle created at %s\", wrap)\n296|     return wrap\n297| \n298| \n299| # ---------- Main train routine ----------\n300| def main():\n301|     _setup_logging()\n302|     ap = argparse.ArgumentParser()\n303|     ap.add_argument(\"--base-model\", default=BASE_MODEL)\n304|     ap.add_argument(\"--train-file\", required=True)\n305|     ap.add_argument(\"--val-file\", default=None)\n306|     ap.add_argument(\"--out-dir\", default=\"out/monGARS_dolphin_multimodule\")\n307|     ap.add_argument(\"--epochs\", type=int, default=2)\n308|     ap.add_argument(\"--lr\", type=float, default=1.5e-4)\n309|     ap.add_argument(\"--per-device-bs\", type=int, default=1)\n310|     ap.add_argument(\"--grad-accum\", type=int, default=8)\n\nAcceptance Criteria:\n- No NotImplementedError/pass; function performs full intended work.\n- Proper error handling and input validation; avoid generic except.\n- Deterministic behaviour and idempotency where applicable.\n- Unit tests pass for this scope.\n\nDeliverables:\n- Updated implementation of \"LLM2Vec.embed\".\n- Short rationale (2–4 bullets) explaining key decisions.\n"}
{"file": "scripts/train_monGARS_unsloth.py", "line": 24, "function": null, "signature": null, "prompt": "You are a senior engineer. Implement the missing logic near the specified line.\n\nConstraints:\n- Keep behaviour backward-compatible unless tests specify otherwise.\n- Use clear types, docstrings, and precise exceptions.\n- Add minimal unit tests if behaviour isn't covered.\n\nContext (numbered):\n 1| #!/usr/bin/env python3\n 2| \"\"\"Fine-tune Dolphin (Llama3.1-8B) with Unsloth on the multi-module dataset.\"\"\"\n 3| from __future__ import annotations\n 4| \n 5| import argparse\n 6| from pathlib import Path\n 7| \n 8| import torch\n 9| from transformers import (\n10|     AutoTokenizer,\n11|     DataCollatorForLanguageModeling,\n12|     Trainer,\n13|     TrainingArguments,\n14| )\n15| from unsloth import FastLanguageModel\n16| \n17| from datasets import load_dataset\n18| \n19| BASE_MODEL = \"dphn/Dolphin3.0-Llama3.1-8B\"\n20| SYSTEM_PROMPT = (\n21|     \"You are monGARS internal assistant. Follow the module contract indicated by \"\n22|     \"tags like [MOD=...].\"\n23| )\n24| \n25| \n26| def load_jsonl_as_dataset(dataset_dir: Path):\n27|     data_files: dict[str, str] = {}\n28|     train_file = dataset_dir / \"train.jsonl\"\n29|     val_file = dataset_dir / \"val.jsonl\"\n30|     if train_file.exists():\n31|         data_files[\"train\"] = str(train_file)\n32|     if val_file.exists():\n33|         data_files[\"validation\"] = str(val_file)\n34|     if not data_files:\n35|         raise SystemExit(f\"No dataset files found in {dataset_dir}.\")\n36|     return load_dataset(\"json\", data_files=data_files)\n37| \n38| \n39| def build_prompt(example: dict) -> dict[str, str]:\n40|     instruction = example.get(\"instruction\", \"\")\n41|     input_section = example.get(\"input\", \"\")\n42|     if input_section:\n43|         user_block = f\"{instruction}\\n\\n[INPUT]\\n{input_section}\"\n44|     else:\n45|         user_block = instruction\n46|     assistant_output = example.get(\"output\", \"\")\n47|     prompt = (\n48|         \"<|im_start|>system\\n\"\n49|         f\"{SYSTEM_PROMPT}<|im_end|>\\n\"\n\nDeliverables:\n- Updated code with a complete implementation.\n- Brief note explaining rationale and complexity assumptions.\n", "title": "Implement missing logic near L24 in scripts/train_monGARS_unsloth.py"}

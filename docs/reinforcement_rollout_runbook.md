# Reinforcement Learning Rollout Runbook

> **Last updated:** 2025-10-24 _(auto-synced; run `python scripts/update_docs_metadata.py`)_

This runbook documents how operators monitor, approve, and roll back
reinforcement-learning (RL) experiments generated by the evolution engine. It
supplements the RAG governance guide and focuses on the approval registry
introduced for RL artefacts.

## Overview

- **Telemetry** – `ReinforcementLearningLoop` now emits OpenTelemetry spans and
  structured metrics via the `reinforcement.loop.*` namespace. Metrics feed the
  observability pipeline and are snapshotted to
  `models/encoders/reinforcement_observability.json`, correlating approvals,
  energy usage, and replica utilisation for dashboards and alerts.【F:monGARS/core/reinforcement_observability.py†L1-L168】【F:monGARS/core/long_haul_validation.py†L120-L470】
- **Approval Registry** – Every RL rollout creates or reuses an approval
  request stored in `models/encoders/operator_approvals.json` via
  `OperatorApprovalRegistry`. Requests are auto-approved only when the configured
  policy passes (e.g., evaluation accuracy above the platform threshold).
- **Evolution Orchestrator** – The orchestrator instantiates
  `ReinforcementLoop` with the approval registry so reasoning adapters cannot be
  published until an operator signs off or a policy approves them automatically.
- **Carbon-Aware Rollout Policy** – Evolution training cycles consult the
  sustainability dashboard before executing, logging deferrals when carbon
  intensity or approval backlogs exceed the configured guardrails.【F:modules/evolution_engine/sustainability.py†L1-L235】【F:modules/evolution_engine/orchestrator.py†L260-L360】

## Daily Checklist

1. **Review Pending Approvals**
   - Location: `models/encoders/operator_approvals.json` (also surfaced in the
     Django operator console).
   - Action: Approve requests with acceptable metrics or reject those requiring
     re-training. Each record captures the evaluation summary for auditing.
2. **Verify Telemetry Streams**
   - Dashboard: `llm.reinforcement.loop` panel in Grafana (or equivalent).
   - File store: confirm the most recent entry in
     `models/encoders/reinforcement_observability.json` reflects the latest
     validation run and includes replica load metrics.
   - Metrics: `reinforcement.loop.batch.success_rate`,
     `reinforcement.loop.summary.average_reward`,
     `reinforcement.loop.summary.failures`, and the derived replica counters
     persisted by the observability store.
   - Alerts: Investigate gaps or sudden drops in throughput before approving
     new artefacts.
3. **Audit Worker Scaling**
   - Check Ray cluster metrics to confirm adaptive scaling decisions align with
     recent batch telemetry.
4. **Review Carbon-Aware Deferrals**
   - Inspect `carbon_policy.*` log entries to confirm deferred training cycles
     align with sustainability expectations and capture follow-up actions when
     high-emission periods persist.【F:modules/evolution_engine/orchestrator.py†L260-L360】

## Long-Haul Validation Service

- The evolution engine now accepts a `ResearchLongHaulService` instance that
  schedules long-haul validation runs whenever training cycles complete. The
  service can operate with or without the distributed scheduler and records the
  latest summary for downstream dashboards.
- Configuration lives in the main settings module:
  - `RESEARCH_LONG_HAUL_ENABLED` toggles the service (defaults to enabled).
  - `RESEARCH_LONG_HAUL_INTERVAL_SECONDS` and
    `RESEARCH_LONG_HAUL_JITTER_SECONDS` control the cadence when the periodic
    loop is started.
  - `RESEARCH_LONG_HAUL_CYCLES`, `RESEARCH_LONG_HAUL_EPISODES_PER_CYCLE`, and
    `RESEARCH_LONG_HAUL_COOLDOWN_SECONDS` still govern the validator itself.
- Operators can call `ResearchLongHaulService.schedule_once(reason="manual")`
  from the Django console or a maintenance shell to trigger an ad-hoc run. The
  service deduplicates concurrent requests and logs
  `research.longhaul.validation_completed` when the summary is ready.

## Approving a Rollout

1. Inspect the pending request (JSON record or console view). Confirm:
   - `metrics.reasoning_accuracy` meets the deployment threshold.
   - The associated dataset or adapter path matches the intended experiment.
2. Approve via the console or run:

   ```bash
   python - <<'PY'
   from monGARS.core.operator_approvals import OperatorApprovalRegistry

   registry = OperatorApprovalRegistry("models/encoders/operator_approvals.json")
   request_id = "<REQUEST_ID>"
   registry.approve(request_id, operator="<your-name>", notes="looks good")
   PY
   ```

3. Re-run the RL pipeline (or wait for the next scheduled cycle). Approved
   requests allow `_rollout_to_manifest` to update manifests and propagate the
   adapter to Ray Serve.

## Rejecting or Rolling Back

1. To reject, annotate the request with `registry.approve(..., notes="rejected")`
   after sanitising the adapter. The existing manifest remains active.
2. To roll back a deployed adapter, restore the previous manifest entry from
   version control or redeploy a prior approved artefact via the Django console.
3. Record the rollback in the incident log so subsequent reviews capture the
   context.

## Troubleshooting

- **Missing Telemetry** – Confirm the process runs with OpenTelemetry enabled.
  The loop falls back gracefully when tracing is unavailable but logs a warning
  (`reinforcement.loop.tracing_unavailable`).
- **Approval Deadlock** – If approved artefacts do not roll out, ensure the
  fingerprint has not changed (identical metrics and artefact paths must be
  provided). Re-run the RL cycle if necessary to regenerate matching payloads.
- **Unexpected Auto-Approvals** – Review the configured policy passed to
  `EvolutionOrchestrator`. Adjust thresholds or replace the policy callable to
  tighten criteria.

## References

- `modules/neurons/training/reinforcement_loop.py`
- `modules/evolution_engine/orchestrator.py`
- `monGARS/core/operator_approvals.py`

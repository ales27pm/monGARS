# monGARS Module Interaction Analysis

> **Last updated:** 2025-10-24 _(auto-synced; run `python scripts/update_docs_metadata.py`)_

## Overview
monGARS layers a FastAPI surface over a cognition core that composes curiosity detection, neuro-symbolic reasoning, adaptive personality shaping, and mimicry-driven style control before persisting each interaction. The `Orchestrator` wires these collaborators together, enriching user prompts with image captions, curiosity hints, and reasoning output prior to invoking the LLM, then shapes the reply through personality analysis and mimicry before logging telemetry.【F:monGARS/core/orchestrator.py†L21-L185】 The higher-level `ConversationalModule` expands on this flow by coordinating memory services, speech synthesis, and evolution feedback loops so that conversation state feeds downstream self-improvement routines.【F:monGARS/core/conversation.py†L25-L164】

## Memory, Persistence, and Retrieval
`Hippocampus` provides a hybrid memory tier: it hydrates per-user histories from the SQL persistence layer (or falls back to direct repository queries), maintains TTL-governed in-memory deques, and optionally schedules periodic flushes back to long-term storage.【F:monGARS/core/hippocampus.py†L47-L193】 Memory writes are deduplicated and normalized through the `PersistenceRepository`, which encapsulates retryable SQLAlchemy sessions, deterministic embedding payload construction, and llm2vec vector normalization so downstream similarity search works even when optional dependencies are missing.【F:monGARS/core/persistence.py†L66-L200】 Together these components allow conversational flows to reuse recent context, enrich prompts, and curate experience samples for later training.

## Personality Adaptation and Style Control
The `AdaptiveResponseGenerator` caches per-user personality traits, coordinating with the `PersonalityEngine` and a shared `StyleFineTuner` so repeated calls reuse derived traits while still allowing stylistic rewrites for each utterance.【F:monGARS/core/dynamic_response.py†L31-L190】 By keeping fingerprints of recent interactions and storing cache hits with TTL semantics, the generator can efficiently tailor the LLM’s base text to the user’s tone preferences without blocking on redundant analysis. Mimicry modules then layer additional voice cloning atop these adapted responses in both the orchestrator and conversational flows, ensuring consistent personalization.

## Self-Improvement and Evolution Feedback
After finalizing each response, the conversation pipeline persists the augmented prompt, LLM output, speech turn, and personality traits, then forwards the stored memory item into the `EvolutionEngine`.【F:monGARS/core/conversation.py†L107-L164】 The evolution subsystem monitors hardware telemetry, records curated memory samples with expiry-aware pruning, and exposes diagnostics for scaling, energy tracking, and retraining triggers so curation data powers future adapter updates.【F:monGARS/core/evolution_engine.py†L50-L176】 This closed loop is what lets monGARS treat day-to-day interactions as fuel for reinforcement and adapter fine-tuning tasks across heterogeneous hardware clusters.

## API Boundary and Dependency Injection
FastAPI routes resolve cognition dependencies lazily, allowing tests to override repositories while production instances share singleton modules. Authentication flows leverage the `SecurityManager`, while chat endpoints request `ConversationalModule` instances that automatically bind personality and dynamic response engines. The API surface also exposes peer-to-peer telemetry and registration flows, coordinating with persistence, personality, and adaptive response components without recreating heavy objects on every request.【F:monGARS/api/web_api.py†L38-L149】 This dependency wiring is what enables the orchestration stack to remain reusable both for synchronous REST calls and WebSocket-driven sessions.

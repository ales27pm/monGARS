# syntax=docker/dockerfile:1.6

ARG RAY_IMAGE=rayproject/ray:2.9.3-py311-cu121
ARG TORCH_WHEEL_INDEX=https://download.pytorch.org/whl/cu121
ARG TORCH_VERSION=2.3.0+cu121
ARG RAY_SERVE_VERSION=2.9.3
ARG VLLM_VERSION=0.4.2

# Ray image with CUDA userland libs (override build args to bump versions).
FROM ${RAY_IMAGE}

ENV PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PIP_DEFAULT_TIMEOUT=1200

# Elevate to root to install system level dependencies, then drop back to the
# default non-root user for runtime execution.
USER root

# System dependencies for pulling model weights and executing multimedia-aware
# pipelines from Ray Serve deployments.
RUN apt-get update \
    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        ca-certificates \
        curl \
        ffmpeg \
        git \
        git-lfs \
        libgl1 \
        unzip \
        wget \
    && rm -rf /var/lib/apt/lists/*

RUN git lfs install --system

# Bundle the vendored llm2vec build so we can upgrade transformers without the
# dependency resolver rejecting newer releases required by Ray Serve adapters.
COPY vendor/llm2vec_monGARS /tmp/llm2vec_monGARS

# Faster, resilient installs (cache + retries) with CUDA wheels bundled directly
# in the PyTorch wheel to avoid long dependency chains.
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install -U --retries 5 pip \
    && pip install --retries 5 --extra-index-url ${TORCH_WHEEL_INDEX} \
        torch==${TORCH_VERSION} \
        "ray[serve]==${RAY_SERVE_VERSION}" \
        "accelerate>=0.33.0" \
        "peft>=0.13.2" \
        "sentencepiece>=0.2.0" \
        "vllm==${VLLM_VERSION}" \
        "xformers==0.0.26.post1" \
        "transformers>=4.44.2" \
    && pip install --retries 5 /tmp/llm2vec_monGARS \
    && rm -rf /tmp/llm2vec_monGARS

# Return to the default ray user for runtime safety.
USER ray

ENV PYTHONUNBUFFERED=1 \
    RAY_DASHBOARD_ADDRESS=http://0.0.0.0:8265

CMD ["serve","start","--http-host","0.0.0.0","--http-port","8000"]
